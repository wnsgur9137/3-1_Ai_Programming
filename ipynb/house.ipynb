{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "421ba2d5",
      "metadata": {
        "id": "421ba2d5"
      },
      "source": [
        "# 15장 실제 데이터로 만들어 보는 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e21b2a",
      "metadata": {
        "id": "b2e21b2a"
      },
      "source": [
        "## 데이터 파악하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a3fbd7bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a3fbd7bd",
        "outputId": "4c4997dc-1f74-4ff0-9304-19e692e790e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                60        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 30)                330       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 40)                1240      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 41        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,671\n",
            "Trainable params: 1,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 39087943680.0000 \n",
            "Epoch 1: val_loss improved from inf to 37350825984.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 1s 9ms/step - loss: 38917419008.0000 - val_loss: 37350825984.0000\n",
            "Epoch 2/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 38684295168.0000\n",
            "Epoch 2: val_loss improved from 37350825984.00000 to 36955103232.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 38684295168.0000 - val_loss: 36955103232.0000\n",
            "Epoch 3/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 37687664640.0000\n",
            "Epoch 3: val_loss improved from 36955103232.00000 to 35765366784.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 37955866624.0000 - val_loss: 35765366784.0000\n",
            "Epoch 4/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 35979681792.0000\n",
            "Epoch 4: val_loss improved from 35765366784.00000 to 32649902080.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 35979681792.0000 - val_loss: 32649902080.0000\n",
            "Epoch 5/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 29282881536.0000\n",
            "Epoch 5: val_loss improved from 32649902080.00000 to 25935212544.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 31248214016.0000 - val_loss: 25935212544.0000\n",
            "Epoch 6/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 22874263552.0000\n",
            "Epoch 6: val_loss improved from 25935212544.00000 to 15320717312.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 22394408960.0000 - val_loss: 15320717312.0000\n",
            "Epoch 7/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 15249098752.0000\n",
            "Epoch 7: val_loss improved from 15320717312.00000 to 5257569280.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 11290230784.0000 - val_loss: 5257569280.0000\n",
            "Epoch 8/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 4296184320.0000 \n",
            "Epoch 8: val_loss improved from 5257569280.00000 to 2045976960.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 3946489088.0000 - val_loss: 2045976960.0000\n",
            "Epoch 9/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2556390656.0000\n",
            "Epoch 9: val_loss improved from 2045976960.00000 to 2035406720.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2556390656.0000 - val_loss: 2035406720.0000\n",
            "Epoch 10/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2523670528.0000\n",
            "Epoch 10: val_loss improved from 2035406720.00000 to 2012050816.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2523670528.0000 - val_loss: 2012050816.0000\n",
            "Epoch 11/2000\n",
            "18/28 [==================>...........] - ETA: 0s - loss: 2781019136.0000\n",
            "Epoch 11: val_loss improved from 2012050816.00000 to 2010138368.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2514281984.0000 - val_loss: 2010138368.0000\n",
            "Epoch 12/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2551462400.0000\n",
            "Epoch 12: val_loss improved from 2010138368.00000 to 2004365312.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2519941120.0000 - val_loss: 2004365312.0000\n",
            "Epoch 13/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2531515392.0000\n",
            "Epoch 13: val_loss did not improve from 2004365312.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2512864512.0000 - val_loss: 2006942336.0000\n",
            "Epoch 14/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2523083008.0000\n",
            "Epoch 14: val_loss did not improve from 2004365312.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2523083008.0000 - val_loss: 2009154176.0000\n",
            "Epoch 15/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2651345408.0000\n",
            "Epoch 15: val_loss improved from 2004365312.00000 to 1987743872.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2507542016.0000 - val_loss: 1987743872.0000\n",
            "Epoch 16/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2505431296.0000\n",
            "Epoch 16: val_loss did not improve from 1987743872.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2513094912.0000 - val_loss: 2001129600.0000\n",
            "Epoch 17/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2506806784.0000\n",
            "Epoch 17: val_loss did not improve from 1987743872.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2506806784.0000 - val_loss: 2002604928.0000\n",
            "Epoch 18/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2507501312.0000\n",
            "Epoch 18: val_loss improved from 1987743872.00000 to 1978811776.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2515768320.0000 - val_loss: 1978811776.0000\n",
            "Epoch 19/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2476742144.0000\n",
            "Epoch 19: val_loss did not improve from 1978811776.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2521775360.0000 - val_loss: 1989840896.0000\n",
            "Epoch 20/2000\n",
            "20/28 [====================>.........] - ETA: 0s - loss: 2737926144.0000\n",
            "Epoch 20: val_loss improved from 1978811776.00000 to 1976139520.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2502010624.0000 - val_loss: 1976139520.0000\n",
            "Epoch 21/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2739606528.0000\n",
            "Epoch 21: val_loss improved from 1976139520.00000 to 1975155200.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2503189504.0000 - val_loss: 1975155200.0000\n",
            "Epoch 22/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 3663344128.0000\n",
            "Epoch 22: val_loss did not improve from 1975155200.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2519828736.0000 - val_loss: 1975229952.0000\n",
            "Epoch 23/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 3377570816.0000\n",
            "Epoch 23: val_loss improved from 1975155200.00000 to 1964090624.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2500496640.0000 - val_loss: 1964090624.0000\n",
            "Epoch 24/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2503112704.0000\n",
            "Epoch 24: val_loss did not improve from 1964090624.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2503112704.0000 - val_loss: 1965180672.0000\n",
            "Epoch 25/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2541388032.0000\n",
            "Epoch 25: val_loss improved from 1964090624.00000 to 1956054784.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2541885696.0000 - val_loss: 1956054784.0000\n",
            "Epoch 26/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2519805440.0000\n",
            "Epoch 26: val_loss did not improve from 1956054784.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2494958848.0000 - val_loss: 1961694720.0000\n",
            "Epoch 27/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 4231506688.0000\n",
            "Epoch 27: val_loss improved from 1956054784.00000 to 1954947840.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2515501312.0000 - val_loss: 1954947840.0000\n",
            "Epoch 28/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2574233088.0000\n",
            "Epoch 28: val_loss did not improve from 1954947840.00000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2501634304.0000 - val_loss: 1957247872.0000\n",
            "Epoch 29/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2512164608.0000\n",
            "Epoch 29: val_loss improved from 1954947840.00000 to 1954412544.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2501408000.0000 - val_loss: 1954412544.0000\n",
            "Epoch 30/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 3422396416.0000\n",
            "Epoch 30: val_loss improved from 1954412544.00000 to 1949177984.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2492698368.0000 - val_loss: 1949177984.0000\n",
            "Epoch 31/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1548195584.0000\n",
            "Epoch 31: val_loss improved from 1949177984.00000 to 1939543296.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2496672512.0000 - val_loss: 1939543296.0000\n",
            "Epoch 32/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2576690176.0000\n",
            "Epoch 32: val_loss did not improve from 1939543296.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2490951936.0000 - val_loss: 1948328064.0000\n",
            "Epoch 33/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2528549888.0000\n",
            "Epoch 33: val_loss did not improve from 1939543296.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2500775424.0000 - val_loss: 1948752640.0000\n",
            "Epoch 34/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2557756672.0000\n",
            "Epoch 34: val_loss did not improve from 1939543296.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2503394304.0000 - val_loss: 1956779520.0000\n",
            "Epoch 35/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2500432384.0000\n",
            "Epoch 35: val_loss improved from 1939543296.00000 to 1930491904.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2519102464.0000 - val_loss: 1930491904.0000\n",
            "Epoch 36/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 5659838464.0000\n",
            "Epoch 36: val_loss did not improve from 1930491904.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2523293952.0000 - val_loss: 1931206528.0000\n",
            "Epoch 37/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2645714432.0000\n",
            "Epoch 37: val_loss improved from 1930491904.00000 to 1927790080.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2496148736.0000 - val_loss: 1927790080.0000\n",
            "Epoch 38/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2521251584.0000\n",
            "Epoch 38: val_loss did not improve from 1927790080.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2507554816.0000 - val_loss: 1936443136.0000\n",
            "Epoch 39/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2500236288.0000\n",
            "Epoch 39: val_loss improved from 1927790080.00000 to 1925099136.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2500236288.0000 - val_loss: 1925099136.0000\n",
            "Epoch 40/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2082190464.0000\n",
            "Epoch 40: val_loss did not improve from 1925099136.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2501325568.0000 - val_loss: 1942253568.0000\n",
            "Epoch 41/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2504596224.0000\n",
            "Epoch 41: val_loss improved from 1925099136.00000 to 1920506880.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2504596224.0000 - val_loss: 1920506880.0000\n",
            "Epoch 42/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1084386816.0000\n",
            "Epoch 42: val_loss did not improve from 1920506880.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2490223616.0000 - val_loss: 1945756928.0000\n",
            "Epoch 43/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2491492096.0000\n",
            "Epoch 43: val_loss improved from 1920506880.00000 to 1918753536.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2491492096.0000 - val_loss: 1918753536.0000\n",
            "Epoch 44/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2395132416.0000\n",
            "Epoch 44: val_loss improved from 1918753536.00000 to 1918734720.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2485934080.0000 - val_loss: 1918734720.0000\n",
            "Epoch 45/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2493720576.0000\n",
            "Epoch 45: val_loss did not improve from 1918734720.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2493720576.0000 - val_loss: 1937497600.0000\n",
            "Epoch 46/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2568165632.0000\n",
            "Epoch 46: val_loss improved from 1918734720.00000 to 1910057728.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2489663488.0000 - val_loss: 1910057728.0000\n",
            "Epoch 47/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2498337792.0000\n",
            "Epoch 47: val_loss did not improve from 1910057728.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2491170560.0000 - val_loss: 1933061760.0000\n",
            "Epoch 48/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2487503616.0000\n",
            "Epoch 48: val_loss improved from 1910057728.00000 to 1906230656.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2487503616.0000 - val_loss: 1906230656.0000\n",
            "Epoch 49/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1422779136.0000\n",
            "Epoch 49: val_loss did not improve from 1906230656.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2487342080.0000 - val_loss: 1916325120.0000\n",
            "Epoch 50/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2635612928.0000\n",
            "Epoch 50: val_loss did not improve from 1906230656.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2498868480.0000 - val_loss: 1908417024.0000\n",
            "Epoch 51/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2505238272.0000\n",
            "Epoch 51: val_loss did not improve from 1906230656.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2505238272.0000 - val_loss: 1911636864.0000\n",
            "Epoch 52/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2457204736.0000\n",
            "Epoch 52: val_loss did not improve from 1906230656.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2485372928.0000 - val_loss: 1920976896.0000\n",
            "Epoch 53/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2453406976.0000\n",
            "Epoch 53: val_loss did not improve from 1906230656.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2491633408.0000 - val_loss: 1907734272.0000\n",
            "Epoch 54/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1573634560.0000\n",
            "Epoch 54: val_loss did not improve from 1906230656.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2485652736.0000 - val_loss: 1917861888.0000\n",
            "Epoch 55/2000\n",
            "18/28 [==================>...........] - ETA: 0s - loss: 1982019072.0000\n",
            "Epoch 55: val_loss improved from 1906230656.00000 to 1898366080.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2482150656.0000 - val_loss: 1898366080.0000\n",
            "Epoch 56/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1305786368.0000\n",
            "Epoch 56: val_loss did not improve from 1898366080.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2512963584.0000 - val_loss: 1908744960.0000\n",
            "Epoch 57/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2497112064.0000\n",
            "Epoch 57: val_loss did not improve from 1898366080.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2497112064.0000 - val_loss: 1908811904.0000\n",
            "Epoch 58/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2490740224.0000\n",
            "Epoch 58: val_loss improved from 1898366080.00000 to 1896322816.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2490740224.0000 - val_loss: 1896322816.0000\n",
            "Epoch 59/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2489493760.0000\n",
            "Epoch 59: val_loss did not improve from 1896322816.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2489493760.0000 - val_loss: 1906968832.0000\n",
            "Epoch 60/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2524112896.0000\n",
            "Epoch 60: val_loss did not improve from 1896322816.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2495603712.0000 - val_loss: 1903529216.0000\n",
            "Epoch 61/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2531816448.0000\n",
            "Epoch 61: val_loss did not improve from 1896322816.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2486813440.0000 - val_loss: 1898394112.0000\n",
            "Epoch 62/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2521636608.0000\n",
            "Epoch 62: val_loss improved from 1896322816.00000 to 1891033216.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2496015104.0000 - val_loss: 1891033216.0000\n",
            "Epoch 63/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2051407360.0000\n",
            "Epoch 63: val_loss did not improve from 1891033216.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2491969024.0000 - val_loss: 1903179520.0000\n",
            "Epoch 64/2000\n",
            "20/28 [====================>.........] - ETA: 0s - loss: 2571198464.0000\n",
            "Epoch 64: val_loss did not improve from 1891033216.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2505966336.0000 - val_loss: 1893726464.0000\n",
            "Epoch 65/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2505113600.0000\n",
            "Epoch 65: val_loss did not improve from 1891033216.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2496928256.0000 - val_loss: 1892083328.0000\n",
            "Epoch 66/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2493172736.0000\n",
            "Epoch 66: val_loss improved from 1891033216.00000 to 1887663872.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2493172736.0000 - val_loss: 1887663872.0000\n",
            "Epoch 67/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2016032000.0000\n",
            "Epoch 67: val_loss did not improve from 1887663872.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2484616192.0000 - val_loss: 1893542656.0000\n",
            "Epoch 68/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2493006336.0000\n",
            "Epoch 68: val_loss did not improve from 1887663872.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2505454848.0000 - val_loss: 1888545536.0000\n",
            "Epoch 69/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2538596864.0000\n",
            "Epoch 69: val_loss improved from 1887663872.00000 to 1886655744.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2502038016.0000 - val_loss: 1886655744.0000\n",
            "Epoch 70/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2517396736.0000\n",
            "Epoch 70: val_loss did not improve from 1886655744.00000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2490665728.0000 - val_loss: 1887495808.0000\n",
            "Epoch 71/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2474570496.0000\n",
            "Epoch 71: val_loss did not improve from 1886655744.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2480956416.0000 - val_loss: 1887137152.0000\n",
            "Epoch 72/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1856477696.0000\n",
            "Epoch 72: val_loss did not improve from 1886655744.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2488831488.0000 - val_loss: 1904542592.0000\n",
            "Epoch 73/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2508077824.0000\n",
            "Epoch 73: val_loss improved from 1886655744.00000 to 1883322112.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2487269376.0000 - val_loss: 1883322112.0000\n",
            "Epoch 74/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2322801152.0000\n",
            "Epoch 74: val_loss did not improve from 1883322112.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2485938432.0000 - val_loss: 1886805888.0000\n",
            "Epoch 75/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2508372992.0000\n",
            "Epoch 75: val_loss improved from 1883322112.00000 to 1883141888.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2482555392.0000 - val_loss: 1883141888.0000\n",
            "Epoch 76/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1768690048.0000\n",
            "Epoch 76: val_loss did not improve from 1883141888.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2476440832.0000 - val_loss: 1892096128.0000\n",
            "Epoch 77/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2438309632.0000\n",
            "Epoch 77: val_loss did not improve from 1883141888.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2494332416.0000 - val_loss: 1883161728.0000\n",
            "Epoch 78/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2572960512.0000\n",
            "Epoch 78: val_loss did not improve from 1883141888.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2478619904.0000 - val_loss: 1888807680.0000\n",
            "Epoch 79/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 733611072.0000\n",
            "Epoch 79: val_loss improved from 1883141888.00000 to 1882380928.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2487126784.0000 - val_loss: 1882380928.0000\n",
            "Epoch 80/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2527698432.0000\n",
            "Epoch 80: val_loss did not improve from 1882380928.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2477661696.0000 - val_loss: 1916190080.0000\n",
            "Epoch 81/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2494748416.0000\n",
            "Epoch 81: val_loss did not improve from 1882380928.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2492813824.0000 - val_loss: 1887004288.0000\n",
            "Epoch 82/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2484776704.0000\n",
            "Epoch 82: val_loss improved from 1882380928.00000 to 1880447360.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2484776704.0000 - val_loss: 1880447360.0000\n",
            "Epoch 83/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2147952896.0000\n",
            "Epoch 83: val_loss improved from 1880447360.00000 to 1880376832.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2488061696.0000 - val_loss: 1880376832.0000\n",
            "Epoch 84/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2422533632.0000\n",
            "Epoch 84: val_loss did not improve from 1880376832.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2503510784.0000 - val_loss: 1886686464.0000\n",
            "Epoch 85/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2517194496.0000\n",
            "Epoch 85: val_loss did not improve from 1880376832.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2548450816.0000 - val_loss: 1882737024.0000\n",
            "Epoch 86/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2543037184.0000\n",
            "Epoch 86: val_loss improved from 1880376832.00000 to 1879556992.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2485028096.0000 - val_loss: 1879556992.0000\n",
            "Epoch 87/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2001350528.0000\n",
            "Epoch 87: val_loss did not improve from 1879556992.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2508069376.0000 - val_loss: 1879661056.0000\n",
            "Epoch 88/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2516520448.0000\n",
            "Epoch 88: val_loss did not improve from 1879556992.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2495479552.0000 - val_loss: 1881133952.0000\n",
            "Epoch 89/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2478956032.0000\n",
            "Epoch 89: val_loss improved from 1879556992.00000 to 1874978048.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2484455424.0000 - val_loss: 1874978048.0000\n",
            "Epoch 90/2000\n",
            "21/28 [=====================>........] - ETA: 0s - loss: 2139664128.0000\n",
            "Epoch 90: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2501915904.0000 - val_loss: 1885029888.0000\n",
            "Epoch 91/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2159297024.0000\n",
            "Epoch 91: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2497405696.0000 - val_loss: 1875755392.0000\n",
            "Epoch 92/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1486073600.0000\n",
            "Epoch 92: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2523353344.0000 - val_loss: 1877592064.0000\n",
            "Epoch 93/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2531521024.0000\n",
            "Epoch 93: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2486195456.0000 - val_loss: 1885621760.0000\n",
            "Epoch 94/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2507718400.0000\n",
            "Epoch 94: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2507718400.0000 - val_loss: 1875773696.0000\n",
            "Epoch 95/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2549233920.0000\n",
            "Epoch 95: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2521007104.0000 - val_loss: 1882273152.0000\n",
            "Epoch 96/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2478378240.0000\n",
            "Epoch 96: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2478378240.0000 - val_loss: 1883079552.0000\n",
            "Epoch 97/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2499688960.0000\n",
            "Epoch 97: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2483010816.0000 - val_loss: 1876480512.0000\n",
            "Epoch 98/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2472466688.0000\n",
            "Epoch 98: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2504012544.0000 - val_loss: 1875256832.0000\n",
            "Epoch 99/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2033805568.0000\n",
            "Epoch 99: val_loss did not improve from 1874978048.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2484566272.0000 - val_loss: 1897378560.0000\n",
            "Epoch 100/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2410359808.0000\n",
            "Epoch 100: val_loss improved from 1874978048.00000 to 1872308224.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2479690240.0000 - val_loss: 1872308224.0000\n",
            "Epoch 101/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2580579584.0000\n",
            "Epoch 101: val_loss did not improve from 1872308224.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2489879808.0000 - val_loss: 1873637504.0000\n",
            "Epoch 102/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2487726848.0000\n",
            "Epoch 102: val_loss improved from 1872308224.00000 to 1871238656.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2483831296.0000 - val_loss: 1871238656.0000\n",
            "Epoch 103/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2492824576.0000\n",
            "Epoch 103: val_loss did not improve from 1871238656.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2492824576.0000 - val_loss: 1871784576.0000\n",
            "Epoch 104/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2541686272.0000\n",
            "Epoch 104: val_loss improved from 1871238656.00000 to 1870317056.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2488770560.0000 - val_loss: 1870317056.0000\n",
            "Epoch 105/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2564486656.0000\n",
            "Epoch 105: val_loss did not improve from 1870317056.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2493271296.0000 - val_loss: 1871664896.0000\n",
            "Epoch 106/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 2101083904.0000\n",
            "Epoch 106: val_loss did not improve from 1870317056.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2472399360.0000 - val_loss: 1876567296.0000\n",
            "Epoch 107/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2526912768.0000\n",
            "Epoch 107: val_loss did not improve from 1870317056.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2481923072.0000 - val_loss: 1873922176.0000\n",
            "Epoch 108/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2588843520.0000\n",
            "Epoch 108: val_loss did not improve from 1870317056.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2471850752.0000 - val_loss: 1879972864.0000\n",
            "Epoch 109/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2492397312.0000\n",
            "Epoch 109: val_loss improved from 1870317056.00000 to 1868689536.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2489074944.0000 - val_loss: 1868689536.0000\n",
            "Epoch 110/2000\n",
            "20/28 [====================>.........] - ETA: 0s - loss: 2851921408.0000\n",
            "Epoch 110: val_loss did not improve from 1868689536.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2484029696.0000 - val_loss: 1881771392.0000\n",
            "Epoch 111/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2486620928.0000\n",
            "Epoch 111: val_loss did not improve from 1868689536.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2481896192.0000 - val_loss: 1870204416.0000\n",
            "Epoch 112/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2584270336.0000\n",
            "Epoch 112: val_loss did not improve from 1868689536.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2483939328.0000 - val_loss: 1875175936.0000\n",
            "Epoch 113/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2096967424.0000\n",
            "Epoch 113: val_loss did not improve from 1868689536.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2487432192.0000 - val_loss: 1881800832.0000\n",
            "Epoch 114/2000\n",
            "21/28 [=====================>........] - ETA: 0s - loss: 2781168896.0000\n",
            "Epoch 114: val_loss improved from 1868689536.00000 to 1867428224.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2479062272.0000 - val_loss: 1867428224.0000\n",
            "Epoch 115/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2540831232.0000\n",
            "Epoch 115: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2481414400.0000 - val_loss: 1871856384.0000\n",
            "Epoch 116/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2536399872.0000\n",
            "Epoch 116: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2475982848.0000 - val_loss: 1877023616.0000\n",
            "Epoch 117/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2525259264.0000\n",
            "Epoch 117: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2480647936.0000 - val_loss: 1869385344.0000\n",
            "Epoch 118/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2607322624.0000\n",
            "Epoch 118: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2494325248.0000 - val_loss: 1868748544.0000\n",
            "Epoch 119/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2569665792.0000\n",
            "Epoch 119: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2477268992.0000 - val_loss: 1872321664.0000\n",
            "Epoch 120/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2487555328.0000 \n",
            "Epoch 120: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2494458112.0000 - val_loss: 1876496000.0000\n",
            "Epoch 121/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2674978560.0000 \n",
            "Epoch 121: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2477519104.0000 - val_loss: 1874664576.0000\n",
            "Epoch 122/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2182436096.0000\n",
            "Epoch 122: val_loss did not improve from 1867428224.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2479354624.0000 - val_loss: 1887719552.0000\n",
            "Epoch 123/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2482854400.0000\n",
            "Epoch 123: val_loss improved from 1867428224.00000 to 1864624128.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2482854400.0000 - val_loss: 1864624128.0000\n",
            "Epoch 124/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2526016512.0000\n",
            "Epoch 124: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2493010688.0000 - val_loss: 1865444736.0000\n",
            "Epoch 125/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2491852800.0000\n",
            "Epoch 125: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2481054208.0000 - val_loss: 1874864512.0000\n",
            "Epoch 126/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2502038784.0000\n",
            "Epoch 126: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2480560384.0000 - val_loss: 1869118080.0000\n",
            "Epoch 127/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2519097088.0000\n",
            "Epoch 127: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2519097088.0000 - val_loss: 1871208320.0000\n",
            "Epoch 128/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2461988864.0000\n",
            "Epoch 128: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2489257216.0000 - val_loss: 1871095168.0000\n",
            "Epoch 129/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2482964736.0000\n",
            "Epoch 129: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2482964736.0000 - val_loss: 1868476416.0000\n",
            "Epoch 130/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2506146304.0000\n",
            "Epoch 130: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2483712512.0000 - val_loss: 1866891136.0000\n",
            "Epoch 131/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2058369408.0000\n",
            "Epoch 131: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2478007552.0000 - val_loss: 1893611776.0000\n",
            "Epoch 132/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2506746624.0000\n",
            "Epoch 132: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2513211648.0000 - val_loss: 1867792128.0000\n",
            "Epoch 133/2000\n",
            "21/28 [=====================>........] - ETA: 0s - loss: 2505910016.0000\n",
            "Epoch 133: val_loss did not improve from 1864624128.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2467006464.0000 - val_loss: 1894019072.0000\n",
            "Epoch 134/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2471352576.0000\n",
            "Epoch 134: val_loss improved from 1864624128.00000 to 1862636416.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2490344704.0000 - val_loss: 1862636416.0000\n",
            "Epoch 135/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2495366656.0000\n",
            "Epoch 135: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2495366656.0000 - val_loss: 1865015040.0000\n",
            "Epoch 136/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2624259840.0000\n",
            "Epoch 136: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2493443072.0000 - val_loss: 1863114496.0000\n",
            "Epoch 137/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2499945472.0000\n",
            "Epoch 137: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2476673280.0000 - val_loss: 1868441344.0000\n",
            "Epoch 138/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2476874240.0000\n",
            "Epoch 138: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2479519744.0000 - val_loss: 1864752512.0000\n",
            "Epoch 139/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2473914624.0000\n",
            "Epoch 139: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2473914624.0000 - val_loss: 1866774400.0000\n",
            "Epoch 140/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2518533888.0000\n",
            "Epoch 140: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2479970816.0000 - val_loss: 1863592704.0000\n",
            "Epoch 141/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2467180800.0000\n",
            "Epoch 141: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2485122816.0000 - val_loss: 1863614848.0000\n",
            "Epoch 142/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 1680399616.0000\n",
            "Epoch 142: val_loss did not improve from 1862636416.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2477714688.0000 - val_loss: 1867198080.0000\n",
            "Epoch 143/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2495434240.0000\n",
            "Epoch 143: val_loss improved from 1862636416.00000 to 1860960768.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2495434240.0000 - val_loss: 1860960768.0000\n",
            "Epoch 144/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2478297344.0000\n",
            "Epoch 144: val_loss improved from 1860960768.00000 to 1860508544.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2494172672.0000 - val_loss: 1860508544.0000\n",
            "Epoch 145/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2535887360.0000\n",
            "Epoch 145: val_loss did not improve from 1860508544.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2487429120.0000 - val_loss: 1862212096.0000\n",
            "Epoch 146/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2524098816.0000\n",
            "Epoch 146: val_loss did not improve from 1860508544.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2476637696.0000 - val_loss: 1865920512.0000\n",
            "Epoch 147/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2415618304.0000\n",
            "Epoch 147: val_loss improved from 1860508544.00000 to 1859478528.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2490402048.0000 - val_loss: 1859478528.0000\n",
            "Epoch 148/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2483681792.0000\n",
            "Epoch 148: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2477581056.0000 - val_loss: 1876122112.0000\n",
            "Epoch 149/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2541223424.0000\n",
            "Epoch 149: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2479730432.0000 - val_loss: 1863657856.0000\n",
            "Epoch 150/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2509382400.0000\n",
            "Epoch 150: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2490491648.0000 - val_loss: 1860130176.0000\n",
            "Epoch 151/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2466608896.0000\n",
            "Epoch 151: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2481215232.0000 - val_loss: 1868281600.0000\n",
            "Epoch 152/2000\n",
            "19/28 [===================>..........] - ETA: 0s - loss: 2712331008.0000\n",
            "Epoch 152: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2477954048.0000 - val_loss: 1859564416.0000\n",
            "Epoch 153/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2510914304.0000\n",
            "Epoch 153: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2476519424.0000 - val_loss: 1864796288.0000\n",
            "Epoch 154/2000\n",
            "21/28 [=====================>........] - ETA: 0s - loss: 2602188800.0000\n",
            "Epoch 154: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2476981504.0000 - val_loss: 1872534784.0000\n",
            "Epoch 155/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2536912640.0000\n",
            "Epoch 155: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2477960704.0000 - val_loss: 1872950016.0000\n",
            "Epoch 156/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2535341312.0000\n",
            "Epoch 156: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2489636096.0000 - val_loss: 1865465344.0000\n",
            "Epoch 157/2000\n",
            "28/28 [==============================] - ETA: 0s - loss: 2488874496.0000\n",
            "Epoch 157: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2488874496.0000 - val_loss: 1860332672.0000\n",
            "Epoch 158/2000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 2526674688.0000\n",
            "Epoch 158: val_loss did not improve from 1859478528.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2507438080.0000 - val_loss: 1879543808.0000\n",
            "Epoch 159/2000\n",
            "20/28 [====================>.........] - ETA: 0s - loss: 2484566528.0000\n",
            "Epoch 159: val_loss improved from 1859478528.00000 to 1859470336.00000, saving model to ./data/model/house.hdf5\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2493506304.0000 - val_loss: 1859470336.0000\n",
            "Epoch 160/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2147666688.0000\n",
            "Epoch 160: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2506578688.0000 - val_loss: 1873729536.0000\n",
            "Epoch 161/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2555949824.0000\n",
            "Epoch 161: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2503463680.0000 - val_loss: 1866814464.0000\n",
            "Epoch 162/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 3050666496.0000\n",
            "Epoch 162: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2491433216.0000 - val_loss: 1873691136.0000\n",
            "Epoch 163/2000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 2227669504.0000\n",
            "Epoch 163: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2531335424.0000 - val_loss: 1859941376.0000\n",
            "Epoch 164/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2648791296.0000\n",
            "Epoch 164: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2485108736.0000 - val_loss: 1874868224.0000\n",
            "Epoch 165/2000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 12820073472.0000\n",
            "Epoch 165: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2516929280.0000 - val_loss: 1873134720.0000\n",
            "Epoch 166/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2511481600.0000\n",
            "Epoch 166: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2497587712.0000 - val_loss: 1874736384.0000\n",
            "Epoch 167/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2525631488.0000\n",
            "Epoch 167: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2486940416.0000 - val_loss: 1878272384.0000\n",
            "Epoch 168/2000\n",
            "21/28 [=====================>........] - ETA: 0s - loss: 2137971712.0000\n",
            "Epoch 168: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2482136320.0000 - val_loss: 1872683776.0000\n",
            "Epoch 169/2000\n",
            "19/28 [===================>..........] - ETA: 0s - loss: 2695554560.0000\n",
            "Epoch 169: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2473851648.0000 - val_loss: 1862062464.0000\n",
            "Epoch 170/2000\n",
            "16/28 [================>.............] - ETA: 0s - loss: 2635881728.0000\n",
            "Epoch 170: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2471385856.0000 - val_loss: 1871971712.0000\n",
            "Epoch 171/2000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 2266290688.0000\n",
            "Epoch 171: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2488395520.0000 - val_loss: 1860759552.0000\n",
            "Epoch 172/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2568642560.0000\n",
            "Epoch 172: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2505240576.0000 - val_loss: 1866958208.0000\n",
            "Epoch 173/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2664029184.0000\n",
            "Epoch 173: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2473322240.0000 - val_loss: 1866717952.0000\n",
            "Epoch 174/2000\n",
            "18/28 [==================>...........] - ETA: 0s - loss: 2708105216.0000\n",
            "Epoch 174: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2481076992.0000 - val_loss: 1864015360.0000\n",
            "Epoch 175/2000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 2575519488.0000\n",
            "Epoch 175: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2475934208.0000 - val_loss: 1861418240.0000\n",
            "Epoch 176/2000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 2420083968.0000\n",
            "Epoch 176: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2482010880.0000 - val_loss: 1861539456.0000\n",
            "Epoch 177/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2645469952.0000\n",
            "Epoch 177: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2484690432.0000 - val_loss: 1870448256.0000\n",
            "Epoch 178/2000\n",
            "18/28 [==================>...........] - ETA: 0s - loss: 2009659136.0000\n",
            "Epoch 178: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2479757056.0000 - val_loss: 1874854528.0000\n",
            "Epoch 179/2000\n",
            "22/28 [======================>.......] - ETA: 0s - loss: 2659423232.0000\n",
            "Epoch 179: val_loss did not improve from 1859470336.00000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2476767744.0000 - val_loss: 1862417792.0000\n",
            "실제가격: 162000.00, 예상가격: 176775.00\n",
            "실제가격: 179400.00, 예상가격: 166800.56\n",
            "실제가격: 214500.00, 예상가격: 212053.61\n",
            "실제가격: 97000.00, 예상가격: 141161.44\n",
            "실제가격: 260000.00, 예상가격: 239870.88\n",
            "실제가격: 138000.00, 예상가격: 146143.64\n",
            "실제가격: 287000.00, 예상가격: 310015.44\n",
            "실제가격: 274300.00, 예상가격: 232456.12\n",
            "실제가격: 100000.00, 예상가격: 119056.23\n",
            "실제가격: 342643.00, 예상가격: 293428.91\n",
            "실제가격: 147000.00, 예상가격: 159269.53\n",
            "실제가격: 224900.00, 예상가격: 198696.75\n",
            "실제가격: 276000.00, 예상가격: 303373.25\n",
            "실제가격: 143500.00, 예상가격: 155225.53\n",
            "실제가격: 125500.00, 예상가격: 119085.04\n",
            "실제가격: 98000.00, 예상가격: 121532.94\n",
            "실제가격: 112500.00, 예상가격: 133203.89\n",
            "실제가격: 167500.00, 예상가격: 173286.31\n",
            "실제가격: 485000.00, 예상가격: 353747.34\n",
            "실제가격: 140000.00, 예상가격: 162913.89\n",
            "실제가격: 336000.00, 예상가격: 279121.66\n",
            "실제가격: 189000.00, 예상가격: 197734.84\n",
            "실제가격: 176500.00, 예상가격: 192146.28\n",
            "실제가격: 82000.00, 예상가격: 109397.86\n",
            "실제가격: 228950.00, 예상가격: 222381.03\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD6CAYAAABUHLtmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeXhb1Z2w3yPv8i7vidfsiZM4cQKEZVgLgZaB0qELXzvQZUrpMu1Mpy1h+nyFdoYZ6MxXCl1oaWGALsPWFmjLFra2FJKQQMie2I4d74vkVZI3Sef74xzZsiPZsi1bjn3e59Fj6dxzz7myZf3ubxdSSgwGg8FgCIYl2hdgMBgMhvmLERIGg8FgCIkREgaDwWAIiRESBoPBYAiJERIGg8FgCIkREgaDwWAISVhCQghRJ4Q4KITYL4TYq8dsQoidQogq/TNTjwshxH1CiGohxAEhRGXAOjfp+VVCiJsCxrfo9av1uWKiPQwGg8EwN4hw8iSEEHXAVimlPWDsu0CnlPIuIcQOIFNKeasQ4v3APwLvB84B7pVSniOEsAF7ga2ABPYBW6SUXUKIPcCXgd3Ac8B9UsrnQ+0x0bVmZ2fL0tLSKf4aDAaDYXGzb98+u5QyZ/x47AzWvBa4WD9/BHgduFWPPyqV9NklhMgQQhTouTullJ0AQoidwJVCiNeBNCnlLj3+KPBB4PkJ9ghJaWkpe/funcHbMhgMhsWHEOJUsPFwfRISeEkIsU8IcbMey5NStujnrUCefr4UaAg4t1GPTTTeGGR8oj0MBoPBMAeEq0lcIKVsEkLkAjuFEMcCD0oppRBiVut7TLSHFlw3AxQXF8/mZRgMBsOiIixNQkrZpH+2A78DzgbatBkJ/bNdT28CigJOL9RjE40XBhlngj3GX98DUsqtUsqtOTmnmdQMBoPBME0m1SSEEMmARUrZp59fAXwHeBa4CbhL/3xGn/Is8CUhxGMox3WPlLJFCPEi8B8BEUpXALdJKTuFEL1CiG0ox/WNwA8C1gq2x5QYHh6msbGRgYGB6ZxuiBKJiYkUFhYSFxcX7UsxGBYt4Zib8oDf6ajUWODXUsoXhBBvA08IIT4DnAI+ouc/h4psqgbcwKcAtDD4N+BtPe87fic28AXgYSAJ5bB+Xo/fFWKPKdHY2EhqaiqlpaXo92GY50gpcTgcNDY2UlZWFu3LMRgWLWGFwJ5JbN26VY6Pbjp69Chr1qwxAuIMQ0rJsWPHWLt2bbQvxWBY8Agh9kkpt44fXzQZ10ZAnHmYv5nBEH0WjZAwGAxnEJ5BeOcXsMAsHWciRkicgbz++utcffXVADz77LPcddddIed2d3fz4x//eMp73HHHHfz3f//3tK+xubmZ66+/ftrnGxY5J16AZ78Eze9G+0oWPUZIzCO8Xu+Uz7nmmmvYsWNHyOPTFRIzwePxsGTJEp566qk53dewgHDqaHe3I7rXYTBCYi6oq6tjzZo1fPzjH2ft2rVcf/31uN1uQJURufXWW6msrOTJJ5/kpZde4txzz6WyspIPf/jDOJ1OAF544QXWrFlDZWUlv/3tb0fWfvjhh/nSl74EQFtbG9dddx0VFRVUVFTw5ptvsmPHDmpqati0aRNf//rXAfiv//ovzjrrLDZu3Mjtt98+stadd97JqlWruOCCCzh+/HjQ9/LJT36SW265ha1bt7Jq1Sr+8Ic/jFzHNddcw6WXXspll11GXV0d69evB5Tw+9rXvsb69evZuHEjP/iBinDet28fF110EVu2bGH79u20tLQE3dOwCPELB3fnxPMMs85MajedkXz794c50twb0TXXLUnj9r8tn3DO8ePHefDBBzn//PP59Kc/zY9//GO+9rWvAZCVlcU777yD3W7nQx/6EC+//DLJycncfffdfO973+Mb3/gGn/3sZ3n11VdZsWIFH/3oR4Pu8eUvf5mLLrqI3/3ud3i9XpxOJ3fddReHDh1i//79ALz00ktUVVWxZ88epJRcc801/PnPfyY5OZnHHnuM/fv34/F4qKysZMuWLUH3qaurY8+ePdTU1HDJJZdQXV0NwDvvvMOBAwew2WzU1dWNzH/ggQeoq6tj//79xMbG0tnZyfDwMP/4j//IM888Q05ODo8//jjf/OY3eeihh6b66zcsRFy6lmi/ERLRZtEJiWhRVFTE+eefD8AnPvEJ7rvvvhEh4f/S37VrF0eOHBmZNzQ0xLnnnsuxY8coKytj5cqVI+c/8MADp+3x6quv8uijjwIQExNDeno6XV1dY+a89NJLvPTSS2zevBkAp9NJVVUVfX19XHfddVitVkCZsULxkY98BIvFwsqVK1m2bBnHjqkqLZdffjk2m+20+S+//DK33HILsbHq42az2Th06BCHDh3i8ssvB5S2UVBQMOnv0bBIcGshYcxNUWfRCYnJ7vhni/HhnIGvk5OTAZUXcPnll/O///u/Y+b6tYBIIKXktttu43Of+9yY8e9///thrxHqvfjfR7jXUV5ezltvvRX2OYZFhF+TMOamqGN8EnNEfX39yBfir3/9ay644ILT5mzbto2//vWvI+Ybl8vFiRMnWLNmDXV1ddTU1ACcJkT8XHbZZdx///2AujPv6ekhNTWVvr6+kTnbt2/noYceGvF1NDU10d7ezoUXXsjTTz9Nf38/fX19/P73vw/5Xp588kl8Ph81NTWcPHmS1atXT/jeL7/8cn7605/i8XgA6OzsZPXq1XR0dIz8ToaHhzl8+PCE6xgWEX4Nwpiboo4REnPE6tWr+dGPfsTatWvp6uri85///GlzcnJyePjhh7nhhhvYuHHjiKkpMTGRBx54gA984ANUVlaSm5sbdI97772X1157jQ0bNrBlyxaOHDlCVlYW559/PuvXr+frX/86V1xxBf/n//wfzj33XDZs2MD1119PX18flZWVfPSjH6WiooKrrrqKs846K+R7KS4u5uyzz+aqq67iJz/5CYmJiRO+93/4h3+guLiYjRs3UlFRwa9//Wvi4+N56qmnuPXWW6moqGDTpk28+eabU/ulGhYuxnE9b1g0ZTmiWdqhrq6Oq6++mkOHDkXtGiLFJz/5Sa6++uo5y4GI9t/OEAWkhH/LBp8H8jfALW9E+4oWBYu+LIfBYDhDGOhWAgKMJjEPWHSO62hQWlq6ILQIUPkQBsOs4tKmppQ8IyTmAUaTMBgM8wt/+Gv2KvD0w3B/dK9nkWOEhMFgmF/4w1+zVV6Q0SaiixESBoNhfjGiSejQahMGG1XCFhJCiBghxLtCiD/o1w8LIWqFEPv1Y5MeF0KI+4QQ1UKIA0KIyoA1bhJCVOnHTQHjW4QQB/U59wmdnSWEsAkhdur5OwNanxoMhoWKP/w1e8XY14aoMBVN4ivA0XFjX5dSbtIPf1rwVcBK/bgZuB/UFz5wO6rv9dnA7QFf+vcDnw0470o9vgN4RUq5EnhFv16UlJaWYrfbp33+T37yk5GSHQbDvMblgLhkSF2iXhtzU1QJS0gIIQqBDwA/D2P6tcCjUrELyBBCFADbgZ1Syk4pZRewE7hSH0uTUu6SKmnjUeCDAWs9op8/EjB+xiKlxOfzzemeHo+HW265hRtvvHFO9zUYpoXbDslZYNV1wIy5KaqEq0l8H/gGMP7b7U5tUrpHCJGgx5YCDQFzGvXYROONQcYB8qSU/vrRrUBesIsTQtwshNgrhNjb0dER5luaO+rq6li9ejU33ngj69evp6GhIWS57g9+8INs2bKF8vLyoEX8xpOSksI///M/U15ezmWXXYb//V988cX80z/9E1u3buXee+8d00Sourqa973vfVRUVFBZWTlS7iPUNRkMc4rLDtZsSNJCwt018XzDrDJpnoQQ4mqgXUq5TwhxccCh21Bf3PHAA8CtwHdm4yIBpJRSCBE0PVxK+YC+BrZu3TpxCvnzO6D1YGQvLn8DXBW6OxxAVVUVjzzyCNu2bQtZrvvCCy/koYcewmaz0d/fz1lnncXf/d3fkZWVFXJdl8vF1q1bueeee/jOd77Dt7/9bX74wx8CqoqsP/v8jjvuGDnn4x//ODt27OC6665jYGAAn8834TUZDHOK265yJGLjIT7VaBJRJhxN4nzgGiFEHfAYcKkQ4pdSyhZtUhoE/gflZwBoAooCzi/UYxONFwYZB2jT5ij0z/YpvLd5RUlJCdu2bQPGluuurKzk2LFjVFVVAXDfffdRUVHBtm3baGhoGBkPhcViGSk1/olPfII33hgtYRCs70RfXx9NTU1cd911ACQmJmK1Wie8JoNhTnE5lCYBYM00PokoM6kmIaW8DaU1oDWJr0kpPyGEKJBStuhIpA8C/pTiZ4EvCSEeQzmpe/S8F4H/CHBWXwHcJqXsFEL0CiG2AbuBG4EfBKx1E3CX/vnMjN/xJHf8s0VgGe1Q5bpff/11Xn75Zd566y2sVisXX3wxAwMDU9onWAnycAh1TQbDnON2KJ8EKJOTiW6KKjPJk/iVEOIgcBDIBv5djz8HnASqgZ8BXwCQUnYC/wa8rR/f0WPoOT/X59QAz+vxu4DLhRBVwPv06zOeUOW6e3p6yMzMxGq1cuzYMXbt2jXpWj6fb6SXdKgS5IGkpqZSWFjI008/DcDg4CButzvkNRkMc8qQS2VZj2gSNmNuijJTqt0kpXwdeF0/vzTEHAl8McSxh4DT+lNKKfcC64OMO4DLpnKNZwJXXHEFR48e5dxzzwWU8/mXv/wlV155JT/5yU9Yu3Ytq1evHjFPTURycjJ79uzh3//938nNzeXxxx+f9Jxf/OIXfO5zn+Nb3/oWcXFxPPnkkyGvKVRZcoNhVvBnW1sDNInO2uhdj8GUCj/TSUlJGbn7X4gs5L+dIQhN++Bnl8INj8Hqq+C5b8CBx2BHfbSvbMFjSoUbDIb5j78CbKC5aaAHvJ7oXdMixwiJM5yFrEUYFiH+uk2BjmuAfpMrES0WjZBYaGa1xYD5my1CRnwSAZoEGOd1FFkUQiIxMRGHw2G+dM4gpJQ4HI5J+2cbFhhuB8TEQ0Kqep2kI+ZNrkTUWBSd6QoLC2lsbGQ+luwwhCYxMZHCwsLJJxoWDm67imzy5/v4o5yMJhE1FoWQiIuLo6ysLNqXYTAYJiMw2xpGzU1Gk4gai8LcZDAYzhD8FWD9JBmfRLQxQsJgMMwf/BVg/cQnKx+FKc0RNYyQMBgM8we3A5IDhIQQun6T0SSihRESBoNhfuAZhMHesZoE6PpNJk8iWhghYTAY5gd+bSF5XP8Ua5bRJKKIERIGg2F+4B5X3M9PUqZxXEcRIyQMBsP8YHy2tR+r6SkRTYyQMBgM8wO/IEgeJySStE/CVEyICmELCSFEjBDiXSHEH/TrMiHEbiFEtRDicSFEvB5P0K+r9fHSgDVu0+PHhRDbA8av1GPVQogdAeNB9zAYDAuQiTQJn0c5tQ1zzlQ0ia8ARwNe3w3cI6VcAXQBn9HjnwG69Pg9eh5CiHXAx4By4Ergx1rwxAA/Aq4C1gE36LkT7WEwGBYabjsIy2i9Jj9+H4VxXkeFsISEEKIQ+ACqxSi6r/WlwFN6yiOoPtcA1+rX6OOX6fnXAo9JKQellLWoVqVn60e1lPKklHIIeAy4dpI9DAbDQsPtUKYly7ivJZN1HVXC1SS+D3wD8OnXWUC3lNLfCaQRWKqfLwUaAPTxHj1/ZHzcOaHGJ9rDYDAsNFz20yObwNRvijKTCgkhxNVAu5Ry3xxcz7QQQtwshNgrhNhrKr0aDGco47Ot/SQZIRFNwtEkzgeuEULUoUxBlwL3AhlCCH8V2UKgST9vAooA9PF0wBE4Pu6cUOOOCfYYg5TyASnlVinl1pycnDDeksFgmHdMpkkYc1NUmFRISClvk1IWSilLUY7nV6WUHwdeA67X024CntHPn9Wv0cdflarbz7PAx3T0UxmwEtgDvA2s1JFM8XqPZ/U5ofYwGAwLDbc9uCaRmK4c2kaTiAozyZO4FfiqEKIa5T94UI8/CGTp8a8COwCklIeBJ4AjwAvAF6WUXu1z+BLwIip66gk9d6I9DAbDQsLnVUJgfPgrgCUGEjOMJhElptR0SEr5OvC6fn4SFZk0fs4A8OEQ598J3Blk/DnguSDjQfcwGAwLjP4uQAbXJEBnXRshEQ1MxrXBYIg+/mzrYD4J0OXCTWmOaGCEhMFgiD6uEMX9/FhtxtwUJYyQMBgM0cdfATaUuSnJBm7TUyIaGCFhMBiiT6i6TX6MJhE1jJAwGAzRZzKfhNUGw24Y7p+7azIARkgYDIb5gMsOCekQG6LQs8m6jhpGSBgMhujjdpzetjQQk3UdNYyQMBgM0ccdoiSHH6NJRA0jJAwGQ/RxOUI7rWFUgBhNYs4xQsJgMEQftz08c5PRJOYcIyQMBkN0kVJXgJ1AkzDmpqhhhITBYIgug73gGw6dSAcq6ik+xZibooAREgaDIbpMlkjnJ8kU+YsGRkgYDIbo4v/in0iTALBmGk0iChghYTAYoou/bpPfOR0Ka5bRJKKAERIGgyG6TMXcZDSJOWdSISGESBRC7BFCvCeEOCyE+LYef1gIUSuE2K8fm/S4EELcJ4SoFkIcEEJUBqx1kxCiSj9uChjfIoQ4qM+5Twgh9LhNCLFTz98phMiM/K/AYDBElRAVYAeGvWPnWU1PiWgQjiYxCFwqpawANgFXCiG26WNfl1Ju0o/9euwqVP/qlcDNwP2gvvCB24FzUN3mbg/40r8f+GzAeVfq8R3AK1LKlcAr+rXBYFhIuOwQmwTxySNDe+s62XDHizR0ukfnJdlgoAe8nihc5OJlUiEhFU79Mk4/5ASnXAs8qs/bBWQIIQqA7cBOKWWnlLIL2IkSOAVAmpRyl5RSAo8CHwxY6xH9/JGAcYPBsFBwO07TIvY3dDPslRxu7h0d9PssBrrn8OIMYfkkhBAxQoj9QDvqi363PnSnNindI4RI0GNLgYaA0xv12ETjjUHGAfKklC36eSuQF+L6bhZC7BVC7O3o6AjnLRkMhvmC6/S6TbV2F8BYTcI/xziv55SwhISU0iul3AQUAmcLIdYDtwFrgLMAG3DrrF2lugZJCA1GSvmAlHKrlHJrTk7ObF6GwWCING7HaULilEMJh1OdrtHBJG2dNs7rOWVK0U1Sym7gNeBKKWWLNikNAv+D8jMANAFFAacV6rGJxguDjAO0aXMU+mf7VK7XYDCcAbjtp5mb/JqEX1gAAfWbjPN6LgknuilHCJGhnycBlwPHAr68BcpXcEif8ixwo45y2gb0aJPRi8AVQohM7bC+AnhRH+sVQmzTa90IPBOwlj8K6qaAcYMhOPZqePVOVQ/IcGYwrgLswLCX5h7Vge40xzUYc9McExvGnALgESFEDEqoPCGl/IMQ4lUhRA4ggP3ALXr+c8D7gWrADXwKQErZKYT4N+BtPe87Ukr/X/sLwMNAEvC8fgDcBTwhhPgMcAr4yHTfqGGRcPBJ+PN3YctNkF44+XxDdBnuh2HXmAqwjV1upISlGUk0dvXj8fqIjbGYxkNRYlIhIaU8AGwOMn5piPkS+GKIYw8BDwUZ3wusDzLuAC6b7BoNhhH6mtXPrjojJM4EgiTS1dqV9nDhqhz+d089LT0DFNmsqsBfTLzRJOYYk3FtWFj06mC4rrqoXoYhTIIk0tVpf8RFq9RYvd/kJETks67rd0NfW+TWW4AYIWFYWPQGaBKG+Y/fCR2gSdQ5XGRY49hYmAEEcV5HSpPw+eAX18Gf7o7MegsUIyQMC4s+IyTOKFx+ITHqk6hzuCjJSiYvLZH4GMu4MNgICglnq/KHtB6MzHoLFCMkDAuH4X7o71LPjZA4MxgxNwUICbubsiwrMRZBoS2J+jGaRATLhfs/I+1HTTTcBBghYVg49Gl/RJwVuk5F91oM4eGygyUWEpVpyR/+Wpqt6jiV2KyjPgmIrCbhFxJDfdBdH5k1FyBGSBgWDn5/ROFWcLXDkGvi+Ybo49YlOVThZxo6VfhraZYSEsU2K/UON9J/p2/NUppEJO78A7XN9iMzX2+BYoSEYeHgj2wqPk/9NNrE/GdcIp0/09qvSRRnJdM36KHLPawmWG3g88Bg38z37qobTdBrOzzz9RYoRkgYFg5+p3XJueqn8UvMf9z2Mf4IfyRTWdaouQkCwmCTIliao+sU5JVDerHRJCbACAnDwqG3GeJTIW+Dem2ExPxnXHG/Wh3+mm6NA6A4SwmJUw5tOoxk1nVXHWSWQN46aDNCIhRGSBgWDr3NkFagvkjiU42QOBNw2cfmSNhdI/4IUD4JYDTCaUST6JrZvkNuFQKbWQq568BRBZ6hma25QDFCwrBw6GuBtCXKCZpZCt3GJzGv8Q6rBkLjsq3LskeFRGJcDHlpCaPmJr/WMVNNwh/NlFGqTE4+D9hPzGzNBYoREoaFQ28LpC5RzzNLjCYx3/GHsuovfhX+OkCJNjH5KbZZOTUiJCJUCdZ/A+HXJMD4JUJghIRhYeDzak2iQL3OLFVCwiRJzV/G1W3yawuBmgRAsS151NyUmA6ImTuu/TcQmaWQvRIscSbCKQRGSBimxZHmXu7ZeWI0fj3auDpAeiE1QEh4BsBpirfNW8ZVgB0Jf80aKyRKsqy09g4wMOwFSwwkZczc3NRVB3HJSkDFxEH2KqNJhMAICcO0eGJvA/e+UjW2UX008SfSpen26Jml6qcxOc1fxmkS/gim8ULC77xu7ApwXs/U3NRVpz4jOonPRDiFJpzOdIlCiD1CiPeEEIeFEN/W42VCiN1CiGohxONCiHg9nqBfV+vjpQFr3abHjwshtgeMX6nHqoUQOwLGg+5hiD41HU4AnjvYEuUr0fhLcgSam8AIifnMOJ9Erd1NZkD4q5/RMNgAv0QkNInMktHXueugtxH6u2e27gIkHE1iELhUSlkBbAKu1G1J7wbukVKuALqAz+j5nwG69Pg9eh5CiHXAx4By4Ergx0KIGN3x7kfAVcA64AY9lwn2MESZmvZRITEvTE5+TcLvuE4vAsS8ybruH/Ly+nHTon0MfnOTDmuts7tGMq0DOS2hzpo1M01CSvW58N9IgIpwAlXszzCGSYWEVDj1yzj9kMClwFN6/BFUn2uAa/Vr9PHLdO/qa4HHpJSDUspaVHvTs/WjWkp5Uko5BDwGXKvPCbWHIYq4Bj009wxQmmWlzuGeHyan3mZVKC45R72OS1ThsPNEk3jkrTo++T9vj61outhx2yEpE2JUg8xTDtdIpnUgtuR4kuNjRjWJmZqbXHZVIjxQSIxEOBnn9XjC8knoO/79QDuwE6gBuqWUHj2lEdDGYJYCDQD6eA+QFTg+7pxQ41kT7GGIIic7lO34cxctJ8Yi5ofJqa9FOa0tAR9pf4TTPOCtGhWNc6RlHgjU+UJAIt1o+OvpQkIIQXFWcoAmMUNzU2Bkk5/0QkhIN36JIIQlJKSUXinlJqAQdee/ZlavaooIIW4WQuwVQuzt6OiI9uUseKo7VHG1s0ozOW951vwwOfU2jUY2+cmYH7kSHq+PvXXqS+1EWwQK0y0U3I4Ap7USAKXZ1qBTx5QMT8qEYTcMD0xvX/9nIiPAJyEE5K41EU5BmFJ0k5SyG3gNOBfIEELE6kOFQJN+3gQUAejj6YAjcHzcOaHGHRPsMf66HpBSbpVSbs3JyZnKWzJMg5p2FzEWQbEtmfdvKJgfJqfegBwJP5mlqujfdL9MIsSh5l5cQ14AjrcaITGCyz7itK7TkU3jcyT8FGcpIeHzyZnXb+quUz8ziseO+yOcon3DM88IJ7opRwiRoZ8nAZcDR1HC4no97SbgGf38Wf0affxVqW4znwU+pqOfyoCVwB7gbWCljmSKRzm3n9XnhNrDEEVqOpyUZFmJj7WwvTx/fpic+logbSntvQM8uVdbL/3mhCg3lNl9UpmaNhVlcKzVmJtGCCjuV6dzJIKZm0CFwQ55fLT1DYyW5piuX6KrDlLyIX6c1pK7DgZ7lFZqGCEcTaIAeE0IcQD1hb5TSvkH4Fbgq0KIapT/4EE9/0EgS49/FdgBIKU8DDwBHAFeAL6ozVge4EvAiyjh84SeywR7GKJIdbuT5TkpgHIqRt3kNNALQ05ILeDXe+r5+lMHaOh0BwiJ6EY47a7tZFlOMhesyKbO4VZJYYsdn2+MuanO4cKWHE96UlzQ6f5SHfUO92iRv+lqEuMjm/z4I5yMX2IMsZNNkFIeADYHGT+J8k+MHx8APhxirTuBO4OMPwc8F+4ehujh8fqoc7h437q8kbH3byjgtt8e5HBzL+uXps/9RY0k0i3hZL26Iz3c3EtRSakaj6JfwuuTvF3bydUVS1idn4rXJ6npcFK+JAq/p/nEQLfKkNeO6zq7m9Ks4P4IgBKb0jBOdbo5p2iGPSW66qDkvNPHc9eqn+2HYdUV01t7AWIyrg1Tor7TzbBXsibDBzWvgtcTfZOTv9lQasFIaYcjzT2QkguxSVEVEkdbeukb9LBtmY01+amA8UsAo1/wAZrE+EzrQAoyEomxiLGaxHTMTZ4h6GkMrkkkZaqMfaNJjMEICcOUqNHhr+e0/Ap+cR386GxsVU/xN8vSo2dy0m1LZdqSESFxuLlXlwyPboTTLu2POKcsi9LsZOJihBESEFC3KYv+IS8tPQNBE+n8xMVYWJqRpCKcZuK47mkAZHAhAcovYSKcxmCEhGFK+MtxZDmPq5DT+GR4+vP80PFZzun+I0ca7HN/UdrcZBc2nIMeYiyCQ8096liUcyV2neykNMtKfnoicTEWluekcNyEwY6p2+QPbZ1ISIDyS5zqdENsAsSnTK/xULAciUDy1kHHcdXrwgAYIWGYItXtTnJTE4izH4eS8+Fzf4YbHicxPZe7435G4S8vgLd/Dp7BubuovmZIslHbrRzC56/Ipq13ELtzMKolw30+ydt1nZxTNtqec01+qtEkYEwF2NHqr6F9EgBFNiv1/jamSdNMqJtMSOSWg28YHNVTX3uBYoSEYUrUdDjZkC2gp145+oSA1VcS+7nX+G72nTR40uGP/wL3boLdP4Xh/tm/qF4V/lprV1rO1RtVvsTh5l71ZTDknHnV0GlwrLWPnv5hzllmGxlblZ9KS88APe5Ffqfq90lYs0ZyJCbVJGxWutzD9A4MgzVzeo7rroazddIAACAASURBVDqISVAhsMALh1pGhBSgNAkwvSUCMELCEDZSSmranZyTorPa/SGDAEJQdPY1XO3+FrXv/7X6cn7+G3BvBbz5QxhyBV0zIvSp3tYn7S7iYyxcoSOvDjX1RLUa7O5a7Y9YNlaTADjRvsi1CbdDmYziEjnlcJGVHE9aYvDwVz+nhcFOR/B3n1JJdBYL7iEPX/z1u9z1fEBRv+xVIGKMXyIAIyQMYdPhHKR3wMP6WJ2s5g8Z1KgoJwtPdi6HTz8Pn/wj5KyGl74J398Ib9wDg84gK8+Q3mYV2dThoiTLSoY1nmKblSN+TQKgqzby+07CrpMOimxJLM1IgqO/h3srWJOp+hccW+wmp4Bs61q767SWpcEoCqwGO936Tf4+EsB7DT14fZI/neigX2fEE5ugOtWZCKcRjJAwhE1NuzYL+OpVV6/0sWUNTkusK70Abvo9fPpFKKiAl+9Q2kUk8QyprnQ6sslf1qF8SRqHm3tGSy/MsSbh80n21Gp/hNcDO78FXXUUuI6QmhDL8cWeee22j4a/2t2TmppgNBv71Ew0iQAh8U69cnwPDPv4c1VAzbfcdaYabABGSBjCptof2eSuUVqE5fSPT9BaTsXb4O9/C6vfD037IntRzlYAfKkFnHK4KctRXyTrl6ZT53DT64uH5Nw5FxJV7U663MOcU2aDA49D50kARMsBVuWncqJ1FjSqMwldAbZ/yEtr70DQEuHjSUmIJSs5XmsSWTDQo3qbh0t/lzpHNxt6t76Lkiwr6UlxvHi4dXRe3jpVymVgkQtyjREShrCpaXeSHG8h3nHsNFOTnwkT67JXqi/LqfxjT4YOf3WILIa8PpbpO9J1S9IAOOo3Oc1xaQ6/P2JbSRr8+btKk0pbCi3vsTo/lWOtvdGvnBtNdEmOU53hOa39FNms1He6dK6EnFonOX8DqsxSpJS8U9/N1hIb71ubx8tH2hj2+tTxXNOAKBAjJAxhU9PhZEu2B+F2jHVaB+A3Of0xWGJd1grwDumEpgihhcQpTwYAZdmqplS5FhIjEU5zrEnsOulgaUYShfVPq70v/lco2AQt77EmP5XeAQ+tvdGtThs1pNTF/Wwjhf0myrYOpCTLOmpugqlFOAWEv55yuOl0DVFZksH28jx6BzzsPqnNV3mmAVEgRkgYwqam3cm5KW3qRQhNAuADG5Tp57Ty4Vkr1M9IxqDr3tYn3Eo4+H0SuamJ5KYmqKS6zFJVimGOEqSkVP6I80pTEX/5b1hSCau2K23CUc0am3JeL9p8iSEXeAZ0jsTEfSTGU2Kz0tzdjydR3RRMyXkd0EfC74+oLM7kwlU5JMXFjJqc0otV5JVxXgNGSBjCxN+ydH2crpOUG1yTALgilMlpREjURO7CepshNonj3TGkJsSSnRI/cqh8SdpohJP0RVaDmYCaDid25xAfifuzsm1f8q8qn6SgApCsFcrssWiFREC29SmHi+yUeFInCX/1U2Sz4pPQPqw1j6k4r7vqlAaSmMa79d2kJMSyKi+VxLgYLl6dw4uHW1W/CovFNCAKwAiJhcjwQMTvmv0tS8u8dapyZ0ro5k4hTU7JOZCQFllNolfnSGintWqNrihfkk5Vu5PBVN3Tao5MTrtOdhLPMJvrHoTCs2DF+9SBggoAUjuPkJeWsHiFhMufSJetw1/DMzXBaIRT/WCSGpiqJhEQ2VRRlE6MRX1etpfn0943yP5G7ePIXacS6haz30hjhMRC5OEPwAs7Irqkv2ZTtj+yaRKCmpyEgKzlkTc3pY4Nf/WzfmmaKs09rEIt50pI7K7t5B+S/0psXxNcfJt63wCp+SrSquU9VuWlLt4aTgGaxGTVX8fjz6eodSfotaYgJLpVHwn3kIdjrX1UFmeOHLpkTS6xFjFqcsorV+XM++ZB//YoY4TEQsMzBM3vwsnXI7psTYeTWIskoasqpNM6kAlNThHWJLypBTR1958mJPw9G97rSYKY+NHolllESsk7NS3cLH4LRdtg+aWjB/0mp9YDrMlPpardiccfUbOY0HWb+uMyaOsdpCxMfwRATkoCCbEWTnZLsMSF77j2eZXpL7N0JIkuUEikJ8Vx3opsXjzUqrTfXH95DmNyCqd9aZEQ4jUhxBEhxGEhxFf0+B1CiCYhxH79eH/AObcJIaqFEMeFENsDxq/UY9VCiB0B42VCiN16/HHdxhTd6vRxPb5bCFEayTe/IOmsUc1cHNUqLjxCVLc7OSfDiRh2haVJhDQ5Za2A7obI9J2WEvpa6InNRsrT+yMXZiaRlhjLoRanSqqbA02i1u7isv4XyPDY4ZIALcJPQQW0H2VtTgJDHh91DvesX9O8Q2sS9QPKZDQVc5PFIii2WTnV2T+1rOveJvB5IHPUab25OGPMlO3ledQ53Jxoc47eCJkIp7A0CQ/wL1LKdcA24ItCCC1muUdKuUk/ngPQxz4GlANXAj8WQsQIIWKAHwFXAeuAGwLWuVuvtQLoAj6jxz8DdOnxe/Q8w0TYT4w+b3onYsvWdDg5N7VdvZjAaR1IUJNT1gpARqZMhtsB3iHapAqHHC8khBCUL0nn0ByGwe6tbuaLsc/Qv2QblF10+oSCCpBeNsapPsonFqPJye2AmHhO9igBOv7vNhklWVaVUDeVrOuA8Nd367tYlpNMhjV+zJTL1+UhBMrkZLWpUvhGk5hcSEgpW6SU7+jnfag+1EsnOOVa4DEp5aCUshaoRrUgPRuollKelFIOAY8B1wrlabwUeEqf/wjwwYC1HtHPnwIuE2L8rZlhDB2RFxIer486u5sN+ouN3DVhnRfU5JS1XP2MhMlJ50jU6xyJYAlZ5UvSONbSiy9jbpoPxb77CHmim8TLv3m6FgEjzuuSoSosYpHWcHI5wJpNXaeqEBxO3aZAVEKdG2nNDF9b1qZGmVHCO/XdY0xNfnJTE9lSnMkLh7RfwpTnAKbok9Dmns3Abj30JSHEASHEQ0II/299KRAYa9iox0KNZwHdUkrPuPExa+njPXq+IRT2EyrOO3tVxEpgNHT1M+T1UebTFTQTUsM6L6jJyRZBIeHPkehPJTslIWgV0fKlaQx6fDjiCpQjMoImuPHIIRcXtf+S40mbEWUXBp+UUQyJGcS1H6A0K3lx1nBy2yE5izr71MJf/ZTYrLiHvAzFZ05NkxAxnPLYVBJdECEBKsrpSEsvDZ1u3YDohKq9tYgJW0gIIVKA3wD/JKXsBe4HlgObgBbg/83KFYZ3bTcLIfYKIfZ2dHRMfsJCxn5clb9YuhWa9kYkhK+mXUU25fSfHHXohclpJqfENBXhExFNQmk2h/qsI+U4xrNeO69HI5xmz3nd9eefkkU3dRu+HHqS33mtI5xOtC3CGk66blPtFCOb/Ph9GL0iLXzHdVcdZBTxTqPS3CpLMoJO216u+ky8eLhVmVW9g8rPt4gJS0gIIeJQAuJXUsrfAkgp26SUXimlD/gZypwE0AQUBZxeqMdCjTuADCFE7LjxMWvp4+l6/hiklA9IKbdKKbfm5ISO31/w+Hxgr1LluZdWquqoEUggq+5wEoeHxJ7wwl8DCW5yWhGZhLreFhAW9ncmhLRrL8tJITHOwgG3vnOcrRpOQy6se37AX7zrWbbl8onnFlRA22HW5iZR53CNlqleLOgKsKccrrBrNgXiLxneKZOV4zqcG6GuupFM65SEWFbmBteGi7OsrMlPVULCNCACwotuEsCDwFEp5fcCxgsCpl0HHNLPnwU+piOTyoCVwB7gbWCljmSKRzm3n5XKDvEacL0+/ybgmYC1btLPrwdelYu6Ktok9DbBsFtrElvUWARMTjXtTrakOBA+T9hOaz9BTU6RypXoa8aXnEubyztS/XU8MRbBmvw03upUZTtmzS/x9s9JHOrkobiPsSI3ZeK5BRXgHWKLtQ0poWqxNSByORhOtNHWOzhpy9JgFNmSEALahq0qYmkwjN+fTqR751Q3m4oyRpLognHl+nz2nuqiI7HUNCAiPE3ifODvgUvHhbt+VwhxUAhxALgE+GcAKeVh4AngCPAC8EWtcXiALwEvopzfT+i5ALcCXxVCVKN8Dg/q8QeBLD3+VSCyGWILDftxABxJZfSkr1ZtGiMhJDqcnB9GzaZQnGZyylqhtJypVPAMRm8LA4mqC91EETLrl6axt9WLTLLNjpAYdMJf72WXZTNJy89j0tiKgk0ArJKqfPiiyrz2DMJQH11S3clPR5NIiI2hIC2RpqEws64HneC2M5RWzLHWXiqLg5ua/Gwvz0dKeLmqR93QLPIIp9jJJkgp3wCCfeqfm+CcO4E7g4w/F+w8KeVJRs1VgeMDwIcnu0aDxl4FwCf/0ENJ0XF+WLARGmcmJKSUVLc7+XJeE1hilUN8ilxRns83nz7EcwdbWL80fbSGU2fNqMYzHXqb6Y5dAhDSJwEqqe6Xu+oZyi0mYTaExJ4HwO3grsGvcF1ZGHEVtmUQn0J231ESYpcsLiGhfQjtXqVtTccnAcrkVOdO1Gt2jnYgDIY2MdZ6c/BJ2FwS3GntZ01+KsU2Ky8cauWG3HXQsn9a17hQMBnXC4mO43gTMjjYFcufTnTgW1KpPuAziM6wO4foHfCwzFuvvtxj4yc/aRynmZwiVeivr5l2bAihbMmh8JcN74wviLwmMdALb95HS+6F7JcrOGeZbfJzLBbI34il9QAr81IWV3kOnW3dOKSEw3Q0CVBhs9V9+rM4mSah/+YHnEqDqCyaWEgIIbhyfT5v1tgZyFqjzp+NtrtnCEZILCTsJ3AklQGCvgEPdQlrlI+i49i0l6z2RzYNnJyWqcnPGJOTTV3jjPwSQy4Y6KHek0FhZhIJsTEhp67KSyXWIjjly1HZ3pFserTnp9DfxRMpHyfDGseqEA7R0yjYCK0HWZ1rXWSahBISJ92JZKckkJIwqTEjKCVZyeHXb9JC4i/2ZJbnJJNunTzkdnt5HsNeyXuDSlOdyf/QmY4REgsJ+wmqfAVkpyRgEfC6S/d3noFfoqbDiZUBkpwNU3ZaB+KPcvrjwRbVbD6jeGZColdFS1UNpI40GgpFYlwMK3JTODqQBb7hkSS8GTPQA2/+AFZdxW9a8zinzIZlAofoGAoqYNjNOWmdtPcN0uUaisw1zXd0BdgTzsQp1WwaT5HNSpfUf/dJhcQpZEIabzT5QuZHjGdzUSY5qQn8sU3PX8QRTkZILBTcneDqYFdfNpeszqGiKINn6xMhMUPlS0yTmg4nG+P9PSSmr0nYkuPZWpLJG1W6AuhMw2D71DUd7k2e0B/hp3xJOm/3KLNTxExOu34CAz10bP1n6jvdnBOOP8KPzrzeYFH28kWTea01icNdcdP2R4BKqOshBYkIy9w0lFpEp3uYykn8EX4sFsEV6/L4TW0sMi55/kc4STlrZc2NkFgoaKf1wcE8zl+RzYUrczjQ1MNwweYZleeobndynr9mU97UEunGc1apjSMtvbiHPKNCYrofbK1J1A2nh1X7Z/3StNFciUgIif5ueOtHsOZq3nCpAgFh+SP8ZK+G2ESKh9TfbdHUcHLZkcJCtTN22v4IUD4JHxYGY9PCMjfZY1XEfriaBKgoJ9eQpDd1+fzXJNqPwN2lUP1KxJc2QmKhoMNfa+QSzluexYWrsvFJqEvQHbaGXNNa9mSHi4r4ZoizQkbpjC5xS2kmXp9kf0O3EhJDfeBsn95iWpNolbawhET5knRaZBY+ERMZIbHrxzDYAxfvYPfJTtISY1mTnxb++TGxkFeO1XGI9KS4RaRJOPAmZCKxzEiTSE+KIzUxFmdM2sSahJTQfYqT3mxSE2JZOVkOSwDblmWRmhjLMV+R+h+azylabUdU2ZmUvIgvbYTEQqHjOEMiHmt2KblpiVQUZpCaGMsb7mLVurN56mF8rkEPTd39LJP1kLNGReXMAH9UyTunumZe6K+3maHYVNwkhiUk1hak4iWGvoT8mQuJwT7YdT+svQbyN7C7tpOzy7ImTNAKSkEFouUAa/IWUQ0nt53+OPU5CLevdTCEEJRkWemWKROX5nC2gWeA95yZbCrOCN9nBMTHWnjf2jz+1JOj9pjuDc1c0H542iHqk2GExALB23GCk758zl2p7iRiYyxcsCKbp1pz1YRpOK9r7Ur7yJ1GzaZgpFvjWJmbwr5TXQFhsNMXEj1x2cTHWliSkTTp9NTEOMqyk2kWeTMXEsdfgMFe2PZ52noHqLW72DYVU5OfggoY7GWbrY8TbU4WRTEBl4Nei6qnNZU+EsEosSXT4U2e2Nyk/9Z7e9LYXDRxEl0wtpfn8e6grjc6jyvCDjQdpNGylP0tke9PYoTEAmG49RhVPmVq8vM3K3M43JvIcGrRtIRETYcTG70kDNpn5LQOZGtpJvtOdeFLXaoywqcrJPpaaCeL0ixr2Hfw65akUTWUNfP6TUeeVr0Girax66S6i52S09qPdl5viW/AqbW2BY/bjl2mkpM6/fBXP8VZVlqGk5ATmZu0kKiXOZMm0QXjwlU51FpK1It5nHntaz3MvsGlxMxCJwUjJBYCw/0kOBuokUs5Z9nol9WFq1Tl0wbr2mk5r6vbnayNaVQvZui09lNZnEnvgIcaR7/KPJ5uhFNvMw2e8JzWfsqXpKkwWFfH9JOjBvugaqcyNVks7K7tJDUhlnVLpuCP8JO7DiyxrPSq38GiyJdw2WkdTqFshloEQLHNSqcvBTmJJiERNMnsSZPogmGNj2XjquU4yEDOV01ioAdrfws1opi1BWHm6UwBIyQWAo4aBJLhzBWkJ40mChVmWlmWk8yeoVLoqZ+yTbWmw8m25IAGLBFgi76b2+f3S0xHk/B6kM42agbSJs2RCGT9knTqpTa/TVebOPGiKh9drvpi7T7p4Kwy29T9EaDyRXLXkuNUiVoLPvPa54X+LuoHk6bcaCgYJTYrnTIVy7A7dDvcrjq6YrIpzMkMK4kuGNvL8zniLaS/8eAMrnYWaT8KgCd7LbExkf9KN0JiLnHUwKMfjHhPg4EW9SHJLt1w2rELV+bwrF1njU7R5FTT7mJjQotqExmhqImy7GRsyfHs9fslOk9OPQPa1Y6QPpp9mWHlSPgpX5I2KiSm65c4/DtIyYeibXT0DVLT4eKcsmn4I/wUVBDbdoAlaQkLX5NwdwKSxkHrjMJf/RRnWelG3zmHMDnJrjpqvdlTCn0dz2VrczlBMXGOE5HN1o8Qg01KeGWUbpqV9Y2QmCs8g3if+CScfA33oT9EdOnmmvfwScGa9ZWnHbtwVTbvDhcjRcyUhITH66PW7mKFPKW0iAjZOoUQVBZn6ginFSoDurt+aovoHIkWaQtZIjwYWSkJDKbolibTERKDTqh+Gdb5TU3aH7FsBs0SCzaB28G5OQOLQEio31enTJ1yX+tgFKQn0Se0kAhhcvI6ajnpyQk7iS4YGdZ4vNlriZOD0BmB3uwRprN2P70yiZWrIuM3HI8REnPFztuJaTtAv4yn8eCfIrp0f9NRGslh87L8046dU5aFJyaJ9qRlUxISjV39DHm9KrIpQv4IP1tKMjlpd9GbrB2CU/VL6I50bWHmSARSvHQJLqzTExInXgDPAKzzm5o6SY6PYf10/BF+tPP63ORmajqcDHt9019rvqOzrR2kRcTcFGMRxKZoAR1MkxgeINbVSoPMnZEmAbB0tapW3FI1/eoFs4Wv9TAnZBGVJTPQaCfACIm54NhzsPt+HvFu5zXfJlI79kc03DGppxpHUimJcacXuUtOiGVriY19Hi0kfOF9CVW3O1mKnTivO2KRTX62lup8CZduKTpVv4Tube1MyCEreWpVadctzaDOl4O3s25qe4KKakrJg+JtAOyudbCl1DYzO3BeOQgL5aKWYa8cCTtekOgKsJ0ybUaJdIEkZ2jzYTBNQmuoHbH5U0qiC8aWrefik4KGo/NMSEhJRl8VrYnLxvgjI0k4nemKhBCvCSGOCCEOCyG+osdtQoidQogq/TNTjwshxH1CiGohxAEhRGXAWjfp+VVCiJsCxrfoBkbV+lwx0R5nFD2N8MwXOBW/gu9bbiRl+XkUyDaOVUWmb669180Sb9OESTQXrspRxf4GepQPIAxqOpystujWpxFyWvvZsDSduBjBrlYBCelTFxK9zQwTR2ZW/uQNfsbh90sM2cP7PYww6AyIaoqh0zXEiTbn9PIjAolPhuxVFPafABZ4DSetSYjkbJJnGP7qJy1L+8qCaRI6OCEpb/mUkuiCkZ9tozW2AE/rocknzyG+niaSpRNv9uyYmiA8TcID/IuUch2wDfiiEGIdqkvcK1LKlcArjHaNuwrVsnQlcDNwP6gvfOB24BxUg6HbA7707wc+G3DelXo81B5nBl4P/OazeIaHuKnv89x8yVoqz1P9j9/bvTMiW7x36CCJYpisIE5rP3+zMpv3fDrDOUyTU02Hk8pE3Zc6wppEYlwM5UvS2Vc/zQinvhY6RCalOVMP91u/VEU4xfU2hK1VAVD1ojI16aimPbUzyI8YT0EFKV2HibGIhZ15rSvAZmSdbhadLtm5qiZTf0/HaccGO9SNWG7JmojsNWBbQ8HAyXmVz9J8Qv0/z5bTGsIQElLKFinlO/p5H6r16FLgWuARPe0R4IP6+bXAo1KxC8jQ/bC3AzullJ1Syi5gJ3ClPpYmpdyl+1c/Om6tYHucGfzpbqh/k/8X9zm8mcv41PmlpJRtwUsM/Sd3R8T+3HBCldtYujL0h2RdQRpd1jIGRWLYQqK63cmmhGZIK4TE9Blf53i2lmTyXmMPXtvyKfskvD1NNHkzphT+6mdJeiL2uAJifYOqZEO4HHkGknOh+FwAdp3sJCkuho2FEfjdFFQg+lrYbBvieOsCbm7jttOHlaKcGfhwxlGYnYFLJuDsOv1vaW84Qb+MZ83y5RHZK7N0EyWinVfemz/O644alf9Utu6sWdtjSsZUIUQpsBnYDeRJKfWtJq2AP0ZyKdAQcFqjHptovDHIOBPsMf+p/TP8+b+oWXot93dt5V+vWqt8BnFJODPXscZznL9UnX73M1VcTSoLNCYntLnJYhGcvyqfg3IZsnFym6qUkpoOF8tlfcSd1n62lGQy5PHRHlcIPQ0wHP7dmbe7STmtpxDZ5EcIQWxWqXoRrvN6yAUnXtJRTcrvs+ukgy0lmcRFIi5dO68vSW/leNvC1SSG++zYfakRCX/1U5xlpYtUBnrspx0baK+hQeawuTgyDt3Msk1YhOTYgT0RWS8SeFsP04aNoqVLZm2PsD/hQogU4DfAP0kpx3yStQYwq4VnJtpDCHGzEGKvEGJvR8fMv3hnjLMDfvNZvLbl/H3Lhzm7zMaV60dV7JQV57Ippoan900x9HMcDZ1usvrr6I+3gXXif4S/WZnNXs8yZOtB1Yx+AuzOIVz9/eQOnoq4qcmPP6nu6FAuIMMPLZQSi7NVhb9O0/mZUaAEqscR5p5VL4GnH9ZdCyhT3PG2vpn7I/zkK1NhZewpGjr7cQ5Ov93sfGawp41O0iKSbe2n2KaK/Hldpxf5i+utxxG/ZNpJdKehm255Ww/zi12nONzcgyfK0WjpfVV0JC2fsm9uKoQlJIQQcSgB8Ssp5W/1cJs2FaF/+tN5m4CigNML9dhE44VBxifaYwxSygeklFullFtzcnLCeUuzh88HT98C/V08VHA7Lf0WvnX1ujF/xJiis0likFNH99I7MDztrd6ssbPc0ozMWjnp3Au0X8LiG4K2iZ1vNR1OSkUrMdIzo250E5GblkiRLYldPdotFa5fYqCbWG8/rTJz2lVEl5SuwicFXU1V4Z1w+GlIzoGS8/H5JLc+dYC0xDg+clbR5OeGQ2I62JZR5lFmt6oFmnntc9nplGkzLuwXiDU+FldMGqK/a8y49PmwDbfgTS+O2F7YyvDFJrI5sZn/+/QhPnDfG2y44yU+8tO3+M/njvL8wRZae0Jkfs8CHd1Oir2NeHNmz2kN4UU3CeBB4KiU8nsBh54F/BFKNwHPBIzfqKOctgE92mT0InCFECJTO6yvAF7Ux3qFENv0XjeOWyvYHvOXt34I1S9jv+AOvrs/lo9sKWL90nF268KtAKyXJ3j+YEuQRcLjr1V2VlmaSVoyuUkoNzURV44ya0xWx6mmw8lqoS2As6RJAGwpzuTFVu1XCFdI6ES6/sQ8UhOnd4e4tiiHFmy42sLYc8itNIm1fwuWGB59q469p7r41tXryE1NnNb+QSmoIKtPZc4v1KS6mH4HDpk6oxLhwRiOzyR+qHvMWF1jIyn0Y82NjD8CAEsMlpw1fLS4jz99/WLu/dgmPnpWEYMeHw/9tZbP/+odtv3nK2z7j1f4/C/38dM/1bD7pEM12ZoFjh9+lwThIa1k9pzWAOHEoZ0P/D1wUAjhb0rwr8BdwBNCiM8Ap4CP6GPPAe8HqgE38CkAKWWnEOLfgLf1vO9IKf1xa18AHgaSgOf1gwn2mJ807oVXvg1rr2HHqbNIiO3kX7YH8RVkliKTc/ibwTr+550mPnrW1O92pJQcraklHSfkrA7rnLWr19K+O4PM+j3Enf3ZkPOq252sj2tEihjELNSn97Ol1MbT+5vxZuURE67zWjcbismYvg22LDuZfeSRF055lKqXYNgN6z5IQ6ebu184zsWrc/hQ5dLJz50KBRXEHf4deXH9CzMMVkoShrsZjM/EGh+Z8NcRrDaSu3rGDNUcP0QZkBehyKYR8soRVS9RkpVMSVYy125Sn4NBj5cjzb28W9/N/gb1eP6QqnsWYxFcU7GEez4a2S9z+8l3gdFEv9li0r+WlPINIJTB67Ig8yXwxRBrPQQ8FGR8L7A+yLgj2B7zkv5ueOpTkLqEt9Z/m5d/cZRbr1wT/G5TCEThWZxTf4hbajtp7HJTmDm1u6sTbU4y3bWQAGRPbm4CuGhVLu+9tZzzTr3NRPfgNR0urohvQaQvh7gI3i2PY4vOgu1KLCJ7ippESvb0zQgxFkFf0lJW/ePyzAAAIABJREFU9IdRGffI02DNRpacx60P7SPGIviP6zZE3gasndeXZ7Zxoq1wkslnIAM9xEoPpGRHfOnYlCxSOl0MDg2REK+SKzsaVKfGgtIIC4ncdbD/V9DXBqmjcTQJsTFsLs5kc0Bmt905yHsN3fzu3SZ+924TX75sZUTKkfjxtBzCi4X4/CibmwxhICX8/ivQ04TnQz/njpcaKbIl8anzS0OfU7iVzP56Mujj6XebQs8LwV+r7aywqLtqssPTJLaUZnJYrCC5r1YJtRDUtDtZwew5rf2szk8lJSGWOlkQtrlpwKEC5Gz5JTPbPLMEm8+Bb3CCJi1DblX1de3f8ti+Ft6scfCv718bVpOjKZOvhMQ2a8PCNDfpuk2JabkRXzopPQeLkDS3to6MDbSrZEmLrTSym+lse352Cez6ifqMhCA7JYHL1uax4yolqHYeaQ05d6oMDHvJdFbTlVisqgnPIkZIRIJ9D6s7zsv+L4+15HO8rW805DUUhWcD8JGCdn77btOUy3T8tdrO5qR21Xs6LTzTR0JsDMP5m9WL5neDznEPeejs7iJ7uGXWnNZ+YiyCzcUZ7O/PVtm445yPwXDaG7DLNEpyp95lLJDkPNUZr/nUidCTqnfCsBt7yfu5849HOXdZFjecHSFn9WkXlAXpRayVtThcQ3T0TRyBdqbh6lZ5DCm2yCXS+UmzKcHT1qpumpyDHpJcDbjibCqjPZIUboW//x1klMALt8K9G+GN76s+I6FOybRSviSNlw5PIS9nEg419bCKejyzmGntxwiJmdJ2GF7YAcsvo6fyC3xv54nTQl6DsmQzCAvXZDVyssPFe409E88PwOP1sbu2k4qkNmVqmkLv6SXrzgOgu/qtoMdPdrhYIZoRyFnXJEA1IdrT649wmrxUhqe7iVZpY9k0ciQCySlW2ldz3dHQkw4/jbRmceveVLw+yV1/NwtmpkAKKijQ5TlOLLAIJ3ub0pYzciIvJDJzVNa1o0OZIt9r6KaIdrxpEYxsCmT5pfDp5+GTz6nw5Zdvh3vWw+t3hbzRuWJdPvvquyIm/N+raaTI0kFqSUVE1psIIyRmwpALnvyUCmG87qf88LUautxDp4W8BiUhBXLLWe05Tnyshd+90zjx/ADea+zBOeih0NMYtqnJzznrllPjK6CvZnfQ4zUdTtZYdP5G3uxqEqDyJWp86p88HJOTpa+ZNplJkW1mETJFy1REWE9ziD2H++HEi9TlXMorJzr5+vbVEQ3dDEr+RpJ6a0lm4Tmvu/QXeG5+5P0t6TblG+jrVBHy75zqoli0k5gXwcimYJSer7SKz74KJefD6/8J92yAl+9QuVIBXFGeh5TwytHIaBPtNcoSkFy0MSLrTYQREtPFM6j8EPYT8KEHqB2w8vCbdcFDXkNRdBZxLe9wxdocfn+ghSFPeIk5b1bbsYoBrP3NExb2C8bynGROxK4mzfGe8qWMo6bdyWpLIzI2ETJLp7T2dNhcnEEjufiwhCUkkgbbcSbkkhA7gSkvDOLT8xggAU+oQn9VO2HYxX+eWsOWkkxuOq90RvuFRUEFAsk2a/OCq+Hk1mUzli6NvJAQOpG0v0cJifdOdbDE4iA+e1nE9wrK0i1ww6/hlr/CysuV+en7G+D5HdCjNKg1+akU2ZJ46cjMhYSUEm+LbqUa4eKbwTBCQnO4uYe3ahzh+Qaa3oGfXgQHn4RLvwnLLuY/njtKfIwleMhrKArPgsFePrF8gE7XEH86EV62+F9r7LwvR3+JTFCOIxhCCIbyNpHu7cLT1XDa8ZoOFxXxzYic1SMlKGaT1MQ4yvJsdMTkTS4kPIOkenvwJEfAZCEEXQlLSHI2BP+bH3maPks6f/Gs4e6/2zi99qRTRUc4XZjazPG2hVXDaai3nX4SsCZHrm7TCElKSAz1qf/floYaYvDNyU3OGPLXw4f/B770NpRfB3segPs2we+/guiu54p1+bxRbZ9xRn2t3UXhcC3DMUnKNzLLGCGh+flfarnhZ7u4+L9f50evVQfPnPQMwav/Dj9/nyq7/fHfwP9v78zjo6zu/f/+zmQlCdlJQhJICCD7GlABFVERtRWhrdVa9bZW26r1VttevV57y89bb+1itbbWW6lata3Uult3BRdQlrDIKmQhIQkh+x6yzvn9cZ4JI0wSQmYjc96vF6+ZObM85/Bk5vuc7/Y59ydsKKjh3b2V3LJk/OAKrDJ0U67ckEISo8J4afvALqejnT1sK2lgSZIVwziFOoakSQsBKNn50QnPFVS1MIFSrwetXcnNiudAdwpqACOhmpw1Ep6pUegeOYYUxxEqm47zE3cdpXvfm7zWOYfbLpzM+CFqEZw0MakQNYqZ9hLyK5txOLza6ca3tNXSYvd8o0gAwmPoETscraOwupW4Ditb0Ac/oG5JmgArHoXbtsHsb8KOv8OTl7J0cjKd3Q4+OsmLwb7YWlLPJFsp3YmTBhWPPFWMkbD43xXTefDrM0mLjeDXb+9nwf3v860nN/PWbssNVLFTp7199GuYeRXc/ClMuJDuHgf3vraXzIRIvr0we3AHTRwPEXGEHM7jyzNH896+Khrb+m/TkVdSR2ePg1mRlSB2SBi833Xa7AV0qBDqDnwxeN3jUNTXHCGup9YnQWsnc8fGU9CTgqopcOsCc1J/RBe/RSV5JsMoLGkcY6SK3WVfTAdu3v0WIT1t7Iu/gBvPGeQ5HQoikDaTrK582jp7KK3vJz33NEIpRWh7LR1hXpKDEaEzNI4YRxNv7znCGLG69/h6J3E88VnwpQfh8t9DUxlzw0tJiArjnT1DS4XdWlzHJFspEel9ywN4EmMkLCLD7KyYncGam87mgx8v5vuLc9hb0cStf93M4/d9j57HFtPdXAVXr4Er/giROgXzH3mlJ5fy6g4RvZso3cLKOel0djt4fYA2HRsKagm1CxndpZCQDSGDU2YDiB0ZTUnoOCKrP/vCeGldG9kOZ9Da+75OJ3PHJFCk0rB1t0Fz31+g2opiwAM1EhZx6ROIkg6KDn2x0eKBdc9Qp6K55uprhqY6dyqkzSS2pYhwOodNvcQL28qJdjRhj/ZeXzVHZALx0sLL28sZH1qDsoXCSO91Rh0UOUsACDm4jgsmjeL9z6uGJBNQXFxIHC1I6gn1x17BGAk3ZCVF8ZOLJ7HhW6lsT/sl31f/4F/dZzG37ueseH8kazYfoqWjm6b2Lh545yRTXvsiYx5Uf870RB1UHsjl9ElhDbMz4wmpKzglV5OT1uRZZHceoKHlWIvuwuoWJnpJja4/MhMiqYuw0hX7cTm1VOu5pWZ6JmslPFl/Tq1VnQuwdmcJZzRuoDz1AiaN9o5mcL+kzURUD5Pk0LAwEtXNHfzPv/aSFtJCapqHW5m4YItKJEGaya9qYWpkAxI3xicxtZMiepROlS1cx9KpqTS3d7OpyI2S3knQ0NZJWN3n+oGPvqPGSLijpxs+foCQP59PTEclXPkMC+98iR9cNo+W9m7uenEX837+Hlf9aePJp7z2ReY8QCGHt7NyTgZbius5VOvezdDY1sWu8kYW5liSn0MwEnETziZKOti5/VgqbGF1C5OkFBUeCzFpp/zZg0VEiE232if0YyS66stoVeGkJXuoatdyR7RX6wynxqNdvPnqX4mWdiYtuc4zxxgsVvD6nOhyPh8GtRKrXttDe2cXSbZmJMp7O4nwmCTi0MH+LHsVxPspHtEXOUvg0EbOGRtBZKidd06x+nr7oQbOEOtCzgcp6mCMxIlU74cnlsL798IZl8Itm2DK5SRFh/Odc8bxzu3n8uLNC7hi9mhKalv55pljTz7l1R3pcwGBsi1cMVtfab3UR5uOT4tqUQrOTzkKjq4hGYnMaYsAqNr3Se9YQVULU0PLkZSp2hXmQ7JzzqBDhdJWsb/P10hLBXX2JGyecgHF6d1LdFsZ9a2d/O/r+1jUsZ7u8DhCx5/nmWOcypwi4pgXUcaB03wn8e7eSl7fWcFvZ5Rj6z5q/a17B1tUAom2VgASOyv8H484npwl4Ogionwj505M4p09lYPusgA6JjnZVoqKTh1QQ8ZTGCPhxNEDGx6G/ztHC+B89Um48imI+mJDMhFhzph4frFyBjtXXcy9y4dozSNidRfXsi2kx0Vy1rgEXtpe5vYP6JPCGkaE2ZkcYsUtTrL7qztCksbTZosm5Mi23mMV9mY2+S5o7WRudiIHVSrNh/s2EpFHK2kN8+DVaNgIOiOSGSNV/OmjIl7OK2RZ6A5CpnwZ7B4SqhksVvD6DEchRTWtdHT3+GceQ6SpvYt7Xt7FpJRoLml8FuKzYfLl3jtgZAJxNBNDG6GdDYFnJDLPgpBIKFzL0impHGlqZ1f5yXdZcJJXXK9T1H0YMzRGwslrt8G7P9XFMLdsgmkrB3yL3SaeadOQMQ/KtoBSrJyTQXFtG9sOndiAb0NBDfOzEwits8RyTrL7q1tsNhoTZjCh+wCF1S0opWiuKiFKtfo0aO1k6uiRlJCGvd59y/DuHgdxPTV0R3vWDWZLyGKMVPF/HxbytbgDhDvaYKqfpdTTZpJ8tBBxdFFY1erfuZwi97/5OdXNHTyyoAXb4W2w8Dawe7hFuCsjEgmhm2uzrd1XoBmJ0AhdoV24liWTRmG3yaB7OXX1ONhdVstYR6lPY4bGSDiZ9x1YuRq+/lcdaPIlGfN0z5faQi6Zlkp4iO2EAPaRxnYKq1tZmJMENfkQnap3IUMgatx8zpBS1u8rpba1k9GdlpynD/8AnYSH2GmNziK2vVzHhI6jvL6VFOqxx3o2YyUkaRxZ9mpE4PbReyAyHrL95GpykjYTu6OLCVJ+WvZw2lhUy983HeLbC7PJ2b8aolNg5je8e1DL9fIf0y2j6q8aif7IWQI1B4jvqmR+VsKg4xJ7DzeR2n2YENXps3gEGCNxjNGzYcaVPvfFA71FdZRtISYilKVTU/nXzoovuBo+KdRC7wvGJ+q4ySArrd0xMucsQsRB2Z6Nuh2HMyDmB3cTQFjqRELppqO2+ITnSstKCZUeRnioRqKX+CxSqGXV0jEklq2FSZf5z9XkJE2L08y0F592PZzau3q464WdZCZE8qPpbVC0Ds662au6JEBv1XWv6mKg7SSgNxWWonUsnZrCgcoWDtac/E5xa0m9y3c0gHYSIvKEiFSJyG6XsVUiUi4iO6x/l7o8958iUiAi+0XkYpfxZdZYgYjc5TKeLSKbrPF/iEiYNR5uPS6wns/y1KIDjuRJED5Su5yAlXPSaWjrYt3nxyozNxTUkhAVxuSUGN0vyhOKcVYgMeTINvZWNDHRVkpPdJq+mvYDo8bqP/ySAztPeK72cDHguRqJXuKzsOHg+vAPobMZpqzw7OefCgnjICyas0eUs/sU/Nb+5KH38imubeP+lTOI3PQwhMdC7re9f2BnEPfwNoiI661jCiiSJ0HMaChcy0VTdFPCwWhMbC2pZ15kBYhtSPHIwXIyO4m/AMvcjD+olJpl/XsDQESmAFcBU633/FFE7CJiBx4BLgGmAFdbrwX4pfVZ44F64AZr/Aag3hp/0Hrd8MRmg/Q5ULYZgHPGJ5EUHd7rclJK8UlhDWePS8TWVgUdTYPu/uqW6FEcHZHOVJXPms2lTLGVYfNDPMJJzmR9BV1dvOeE55qrdcFbdLLnjQQAG/+of1zG+dnVBPrvIXUGueGHWF9Qw1u7PSdW4012lzey+uMirszNYGFcA+x9FeZ/ByK80K/peJw7ifriwNxFgPZS5CyBog/IiA0flMaEUoq8kjrmRFboLguhXhC+6oMBjYRS6iPgZCs/lgNrlFIdSqmDaJ3r+da/AqVUkVKqE1gDLBcd9V0CPG+9/yngCpfPesq6/zxwgXi1mb+fyZintSk6Wwmx27h85mjWfl5FQ1snB2taqWhsP+ZqAo+4mwBCx+Qyy1ZIQWUDOVLu06yJ40lMHk0zUXRWnSgE1FWvDaZ4uorW6btuKodJX/K/q8lJ2kxGtxcwc3Q0d724030vsQCiu8fBnS/sJCEqjP+6dApseEgrpp35Pd9MYETisfuBaiQAcs7X8ceKHYPSmChvOEplUwfZPSU+TywZSkziVhHZabmjnP6JdMC1tWiZNdbXeCLQoJTqPm78C59lPd9ovX54kjEflKNXMW7lnHS6ehSv7axgQ6GWftRBa+sH1BPuJiAkM5dMqWaO5BNGl1+C1r2IUBcxhsim4hNSgKW5gh7snk8qiEkDu9XaxN9ZTa6kzUS62vj9xSPp6HLwo3/uCOiGf6s/Psiew03ce/lUYrur4bM1urmdr5JAIuMA6xoy0ArpXBl3PiBQsHZQGhNbS+qJpJ2Yo2U+bb4Jp24kHgVygFlABfCAx2Z0CojITSKSJyJ51dVD67DoNzJy9W2pdjlNHT2SiSnRvLStjA35NaTHRTI2cYQ2EmExnquItuISV9o/0I/9aSQAR0IOGeowxS5V5+1dPUR1VNEWluT5Vgs2m95NRMT6P6vJFavyekxHPqsun8KGglpWfzywcp8/KKpu4aH3DnDx1BQumZ4Gnz6iL3gW/MB3k7DZj2X7BfJOIipRn9vCtYPSmNhaUs/MsAqtGHk67CSUUpVKqR6llANYjXYnAZQDruknGdZYX+O1QJyIhBw3/oXPsp6PtV7vbj6PKaVylVK5ycneK/33KiMStK+xLA/QRXsrZmew7VADHx6oZuH4RF2T4cxs8pTnbfQslNi4zL4J5eOAmDti0ieRITVsLzrW6LC4tpVUqaM7KsU7B51/I5x/zyk1S/QaSRMhJAIqPuPK3EwumZbKr9/ez65ByNz6AodDcdeLuwgLsXHv8mnQVgd5T8K0r/j+x9oZvA5kIwE6LlG2GeloPmmNibziepYk6gxHX1/InZKREBHXy9gVgDPz6VXgKiszKRuYAGwGtgATrEymMHRw+1WlfQrrgK9a778eeMXls6637n8VWKtOpY79dCJzfm9RHcAVs0cjAke7elg43qr8rsn3mKsJgLAoZNQURkiHzqrxYUDMHQlj9BfgUH5vMh0Hq1tJlXpssV5qEHfmd+HMm7zz2aeKPQRSpsHeV5Gqvfxi5XSSY8L59zXbaescmmiNJ3l2yyE2H6zjnssmkzIyArb8GbpaYdEPfT+ZyNPISDi6oXg9S6ekDKgx0dLRzedHmsiNqIDQEbp63YecTArss8CnwBkiUiYiNwC/EpFdIrITOB+4HUAptQd4DtgLvAXcYu04uoFbgbeBfcBz1msB7gTuEJECdMzhcWv8cSDRGr8D6E2bHbZk5EJrFTRo3YS02EgW5OgwzNk5idDeBM2DlywdkPQ5AIif6iNcsSWNB6CpfF/vWFFNKylSx4hEz0tfBjQX/T/oPgqPLSZux2P89mszOFjbyr2v7fX3zABd4Hn/G5+zICeRK3Mzteb7xkdh4jKfFnv1MiJBp4fGeriWxtNkzofQKChcy9yx8QNqTOw41IBDQbajRKfR+kBoyJUB6+SVUle7GX7czZjz9fcB97kZfwN4w814EcfcVa7j7cDXBprfsKK3qC6v92rozmWT2FRUpxXvyi276nEjMRe2Pe33eATQK6IU0XiQxqNdxEaGcriympFyFOKDzEhkLYLvf6pbxrzzX5yd9RY/OevH/OrTUs6bmKz9/35CKcU9L++my+HgFyuna1fotmfgaB0sut0/k0oYp39EAyVDrS9CwvW5LVxLiN3GBZNG8daeI3T1OAh107wyr6QOEYhtzocz3FUjeBdTcR1IjJqqt5NWUR3AjIw4bjzXEnSvtjKbPB03GLtQX4FlnGCrfU94NB2RKWRLBdsP1QPHaiQCRkTGl0Qnw1V/h8v/AIe38/291/KDpG3c9cJOKhqPDvx+L/H6rgre21fJHRdNZGxiFPR0wSe/hzFnw5iz/DOpC1fBv73un2MPlpwlUFcI9cUDakxsLannzORubG01Ps9sAmMkAgt7CIye05vhdAI1+8EW6nmfa9IEuONzGH+BZz/3FAlJnsA42xG2lmgj0WnVSPhS4yKgEIE518L31iOjpvCjlt9wv3qQe579mB4/pMXWt3ay6tU9TE+PPSbZu+uf0FQGi+7w+Xx6CY30WfvsIeNs0VG4jkXjk4gItbnt5dTjUGw/1MDSJCtnxw91TMZIBBoZuXBkJ3S5uUqsyddbam9sp2NS/NO3yg325PGMt2sj0dDWyYh2S7M4GHcSriRkw7fegAv+m4ttW7iv4ibeePmvPjt8j0Px1u4jXPvEJhrauvjlV2ZoeVeHA9Y/pK9yJ1zks/mc1iRNgJEZULiWyDA7505IdqsxcaCymZaObuZFHtYDZidhIHO+znyoOLF/kaca+wU8CTnEqiYOlpZRUNVCqljb8GDdSbhis8M5P0JufB8VPpIv77yVmudug073aoae4GhnD89sLOGCBz7ge3/dSkNbFw9cOZMpo612Gwfe1LvcRbcHzIVGwCOiq68Pfgg93Syd6l5jIs/aTWc7DkFUsnY/+hhjJAKNdKuoziUuAUB3J9QVeT5oHYgk6gynlK5y3th1hFSppyc8DsJG+HligYOMnsWIW9azxv4lkvY+heNP5/ZW63uK6uYOHnhnPwvuf5+fvryb2MhQHvnGHD748WKWz7LSkZWCj3+rCxKnBkBzxNOJnCXQ3giHt3PBpFHYhBN6OW0triM5Jpyohv1+SywxRiLQiEnREpZlx8Ul6g+C6vFMY79AxzIS2VLBKzvKSZU6ZKTZRRxPbOxIsq55mGs676apsQH+fCF8+Gu3ehyDIb+ymTuf38nCX67lD+sKyM1K4Lnvns3Ltyzkshlp2sXkpHg9lOd5X1RoODJuMSBQuJb4qDDmZ5+oMbH1UD3zxoxEqj/3T1oxxkgEJhnzeiuve/FwY7+AJj4LxMaMyGpqWzsZE9rovUK605yzxiUy67zlnNtyH+Xpy2Ddz+HF72g53kHg7DT87b9s4aIHP+LlHeV8bW4G799xHquvy2V+doJ7Fcb1D2o3yKxrPLSiIGJEgtaxKVwLwNIpqV/QmKhqaqe07iiLR7VCd7vZSRhcyJivu5I2lh8bczb2SxyCZOnpQkgYxI1lRqRuQ5AqdSYe0Q8/vHAi2ZkZXFJ6HY2Lfgp7XoJ//bC3ct8dSilaOropqm7h5e3lfPkP6/nG6k18VtrA7RdO5JO7lnDfiumMS47u+8CHd0Dh+5aokH8r9U9bcpZo13J74wkaE87svtxIq0WNn3YSZn8YiLgo1eG8gq45oLMhwvv50g4nEseTdeQQIXQT21NvMpv6IdRu43dfn8VlD3/MjYULefzM24nZ9CCHWu18Mu52qls6qW7poLq5g6pmfVvd3MHRrmO7jZzkKH6xcjorZqcTEXqSTRQ3PKTFsubdMPBrDe7JWQIf/wYOfkzm5C8xJU1rTNx0bg55JfWEh9gY21UMiC4U9APGSAQiqdPBHq6NhLN9dfV+nTYXLCSOJ754PaNo0J0vzU6iX7KSolh1+VR+8vxOph/M5WchF/Ot/U/yz93N/L5nJbGRoSTHhJMcHc6szDhGxYTrxzHhZMSPIHdsPDbbIDKTagth7yuw4LYha60HNRnzICxau5wmf4mlU1P43fv5VDd36M6vGXHYa/ZaaoX+SdwwRiIQCQmD0bOOxSWU0jUSc67177x8SWIOtu6jPHROj24POdLEJAbiq3MziAoPob6tk+Soh6jdcQ8/Knie2y6dQ+jCWzx7sA2/04WdZ93s2c8NNkLCIOucL8QlHnovn9d3HmbP4UZuWDQO8vf6pYiud4p+O7KhfzLmwebVOvW1tUp31gyG9FcnVobTfKx+VSa7aUBEhEtd+zlNfgye7yT03bthRBzM9lBwuakCPntWiwrFeKl9ezCRs0TXmtQVMTktm4z4SB75oJCuHsX89AjYVATT/dfGzgSuA5WMXOjpgMpdHlejOy2wjAQHP9a3MSYmMWjsIfCVx7Ua2qu3avfQUGmphjd+rAs+fSkqNJxxadEhIiydktoraTo3qgr8IDTkijESgYqz2V5Znvca+wUyI9O16E7Nfh2fOV168gQaIeFw1d/0zvT5G6DgvVP7nK52ne76+zmw/01YfLf2kxuGTmIOxI455nKaqndn45KjiG2yvvt+aMfhxBiJQCU2XV89l27WP5QRsTofPViw2XrbhjMyzbR7GAphUfCN52DUJFjzTTi08eTfqxTseh7+MA/eWwVjF8DNG+G8n3htukFHb4uOj6Cni9yx8aSMDNea9pV7ISRS9+3yEycjOvSEiFSJyG6XsQQReVdE8q3beGtcRORhESkQkZ0iMsflPddbr88XketdxudaAkYF1nulv2MEFRm5OsOpJl9XWgfbD2WiZSSMq2noRMbBN1/SFx9/+xpUfDbwe0o3w+MXwQs36IuU616Bb/wjOAo6fU3OEuhogvKthNhtvH7bOdx96WSo2qM9CJ7Wdh8EJ7OT+AtwvNLFXcD7SqkJwPscU427BC1ZOgG4CXgU9A8+8DPgTLTA0M9cfvQfBW50ed+yAY4RPGTO1yp15duC84vpjEuYoLVniE7WP/QRsfDMimNuzOOpL4Z/fksbiIZSWP4IfPdDq42EwStkn6s1XSyXU1J0OJFhdr2T8FMRnZMBjYRS6iPgeDWM5cBT1v2ngCtcxp9Wmo1AnKWHfTHwrlKqTilVD7wLLLOeG6mU2mjpVz993Ge5O0bw4CyqC7bMJie9RsLsJDxGbIY2FGKHZ66AhkPHnmtvhHf/W7uW9r8J590JP9iqs5j8eCUbFIxI0FoylpEAoLVGZzb6WTHyVGMSKUopq1acI4AzDy4dKHV5XZk11t94mZvx/o4RPKTNBJuVpRwMjf2Ox2kkjLvJsyTmwLUvQWcLPL1ct3/ZvBoeng0bHoZpX4XbtsH5dwdPhX8gkLMEyrfCUd2Og0or/duPmU3ggcC1tQPwqjzWQMcQkZtEJE9E8qqrq705Fd8SGgmpM/Thvv4IAAAFdklEQVT9YKq2dpI6HSYu00E9g2dJnQbXvADNlfC7GTqtddQUuOkDWPGo2b35g5wloBw6gA1QtVff+jGzCU7dSFRariKsW0s6jHIg0+V1GdZYf+MZbsb7O8YJKKUeU0rlKqVyk5OHWQbQ2AW6P46nJUtPB8JG6EDpqMn+nsnwJHMeXP0sZC2Cq56F61/Tlf4G/5CRC2Exx1xOlXtgRCJEj/LrtE7VSLwKODOUrgdecRm/zspyOgtotFxGbwNLRSTeClgvBd62nmsSkbOsrKbrjvssd8cILhbfBTeuMz5hg3cYd56OUUy6NPiy5wINe6gOYBes1anHVXv17s7P5+VkUmCfBT4FzhCRMhG5AbgfuEhE8oELrccAbwBFQAGwGrgZQClVB/wPugvPFuBeawzrNX+23lMIvGmN93WM4CI8BpLG+3sWBoPBF+ScD42HoLYAqvwnNOTKgL2blFJX9/HUBW5eqwC3ncSUUk8AT7gZzwOmuRmvdXcMg8FgGLY4W3TkPamzGv2c2QSm4tpgMBgCh4RxWi9829P6cQDsJIyRMBgMhkBBRO8mOpv1Yz8JDblijITBYDAEEuMtL3t8VkDUqRgjYTAYDIFE1jm6It7P9RFOjOiQwWAwBBKRcbDs/oCIR4AxEgaDwRB4nHmTv2fQi3E3GQwGg6FPjJEwGAwGQ58YI2EwGAyGPjFGwmAwGAx9YoyEwWAwGPrEGAmDwWAw9IkxEgaDwWDoE2MkDAaDwdAnort7Dx9EpBooAZKAGj9Px58E8/qDee0Q3OsP5rXD0NY/Vil1grTnsDMSTkQkTymV6+95+ItgXn8wrx2Ce/3BvHbwzvqNu8lgMBgMfWKMhMFgMBj6ZDgbicf8PQE/E8zrD+a1Q3CvP5jXDl5Y/7CNSRgMBoNh6AznnYTBYDAYhsiwNBIiskxE9otIgYjc5e/5+BoRKRaRXSKyQ0Ty/D0fbyIiT4hIlYjsdhlLEJF3RSTfuo335xy9RR9rXyUi5da53yEil/pzjt5ERDJFZJ2I7BWRPSLy79b4sD///azd4+d/2LmbRMQOHAAuAsqALcDVSqm9fp2YDxGRYiBXKTXs88VF5FygBXhaKTXNGvsVUKeUut+6SIhXSt3pz3l6gz7WvgpoUUr9xp9z8wUikgakKaW2iUgMsBW4Avg3hvn572ftV+Lh8z8cdxLzgQKlVJFSqhNYAyz385wMXkIp9RFQd9zwcuAp6/5T6C/PsKOPtQcNSqkKpdQ2634zsA9IJwjOfz9r9zjD0UikA6Uuj8vw0n9eAKOAd0Rkq4gEjg6i70hRSlVY948AKf6cjB+4VUR2Wu6oYedqcYeIZAGzgU0E2fk/bu3g4fM/HI2EARYppeYAlwC3WG6JoERpf+rw8qn2z6NADjALqAAe8O90vI+IRAMvAD9USjW5Pjfcz7+btXv8/A9HI1EOZLo8zrDGggalVLl1WwW8hHbBBROVls/W6but8vN8fIZSqlIp1aOUcgCrGebnXkRC0T+Sf1NKvWgNB8X5d7d2b5z/4WgktgATRCRbRMKAq4BX/TwnnyEiUVYgCxGJApYCu/t/17DjVeB66/71wCt+nItPcf44WqxgGJ97ERHgcWCfUuq3Lk8N+/Pf19q9cf6HXXYTgJX29RBgB55QSt3n5yn5DBEZh949AIQAfx/O6xeRZ4HF6O6XlcDPgJeB54Ax6I7AVyqlhl2At4+1L0a7GhRQDHzXxT8/rBCRRcDHwC7AYQ3fjfbND+vz38/ar8bD539YGgmDwWAweIbh6G4yGAwGg4cwRsJgMBgMfWKMhMFgMBj6xBgJg8FgMPSJMRIGg8Fg6BNjJAwGg8HQJ8ZIGAwGg6FPjJEwGAwGQ5/8f0LOQnTGHxzfAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#데이터를 불러 옵니다.\n",
        "df = pd.read_csv(\"/content/house_train.csv\")\n",
        "\n",
        "#카테고리형 변수를 0과 1로 이루어진 변수로 바꾸어 줍니다.(12장 3절)\n",
        "df = pd.get_dummies(df)\n",
        "\n",
        "#결측치를 전체 칼럼의 평균으로 대체하여 채워줍니다. \n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "#업데이트된 데이터프레임을 출력해 봅니다.\n",
        "#df\n",
        "#집 값을 제외한 나머지 열을 저장합니다. \n",
        "cols_train=['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF']\n",
        "X_train_pre = df[cols_train]\n",
        "\n",
        "#집 값을 저장합니다.\n",
        "y = df['SalePrice'].values\n",
        "X_train_pre=X_train_pre.astype(float)\n",
        "y=y.astype(float)\n",
        "#전체의 80%를 학습셋으로, 20%를 테스트셋으로 지정합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_pre, y, test_size=0.2)\n",
        "#모델의 구조를 설정합니다.\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(30, activation='relu'))  # 30 \n",
        "model.add(Dense(40, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "\n",
        "#모델을 실행합니다.\n",
        "model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
        "\n",
        "# 20회 이상 결과가 향상되지 않으면 자동으로 중단되게끔 합니다.\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "# 모델의 이름을 정합니다.\n",
        "modelpath=\"./data/model/house.hdf5\"\n",
        "\n",
        "# 최적화 모델을 업데이트하고 저장합니다.\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "#실행 관련 설정을 하는 부분입니다. 전체의 20%를 검증셋으로 설정합니다. \n",
        "history = model.fit(X_train, y_train, validation_split=0.25, epochs=2000, batch_size=32, callbacks=[early_stopping_callback, checkpointer])\n",
        "# 예측 값과 실제 값, 실행 번호가 들어갈 빈 리스트를 만듭니다.\n",
        "real_prices =[]\n",
        "pred_prices = []\n",
        "X_num = []\n",
        "\n",
        "\n",
        "\n",
        "# 25개의 샘플을 뽑아 실제 값, 예측 값을 출력해 봅니다. \n",
        "n_iter = 0\n",
        "Y_prediction = model.predict(X_test).flatten()\n",
        "for i in range(25):\n",
        "    real = y_test[i]\n",
        "    prediction = Y_prediction[i]\n",
        "    print(\"실제가격: {:.2f}, 예상가격: {:.2f}\".format(real, prediction))\n",
        "    real_prices.append(real)\n",
        "    pred_prices.append(prediction)\n",
        "    n_iter = n_iter + 1\n",
        "    X_num.append(n_iter)\n",
        "#그래프를 통해 샘플로 뽑은 25개의 값을 비교해 봅니다.\n",
        "\n",
        "plt.plot(X_num, pred_prices, label='predicted price')\n",
        "plt.plot(X_num, real_prices, label='real price')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('테스트 loss', model.evaluate(X_test,y_test)/10000000)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXuryYJzgtxM",
        "outputId": "106a75be-5dea-443c-98d5-f78317378d4d"
      },
      "id": "AXuryYJzgtxM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step - loss: 2231532544.0000\n",
            "테스트 loss 223.1532544\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "house.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}